---
title: "ARIMA models"
format:
  html:
    code-fold: true
number-sections: true
---

ARIMA models provide another approach to time series forecasting. </br>
Exponential smoothing and ARIMA models
    are the two most widely used approaches to time series forecasting,
    and provide complementary approaches to the problem. </br>
While
    **exponential smoothing** models
        are based on a description of the trend and seasonality in the data,
    **ARIMA** models
        aim to describe the autocorrelations in the data.

Before we introduce ARIMA models,
    we must first discuss the concept of stationarity
    and the technique of differencing time series.


## Stationarity and differencing {#sec-stationarity-differencing}

A **stationary time series**
    is one whose statistical properties do not depend
    on the time at which the series is observed. </br>
More precisely,
    if $y_t$ is a stationary time series,
    then for all $s$,
    the distribution of $y_t, \dots, y_{t+s}$ does not depend on $t$. </br>
Thus, time series with trends, or with seasonality, are not stationary —
    the trend and seasonality will affect the value of the time series
    at different times. </br>
On the other hand, a white noise series is stationary —
    it does not matter when you observe it,
    it should look much the same at any point in time.

Some cases can be confusing —
    a time series with cyclic behaviour (but with no trend or seasonality)
    is stationary. </br>
This is because the cycles are not of a fixed length,
    so before we observe the series
    we cannot be sure where the peaks and troughs of the cycles will be.

In general, a stationary time series
    will have no predictable patterns in the long-term. </br>
Time plots will show the series to be roughly horizontal
    (although some cyclic behaviour is possible),
    with constant variance.

```{r}
#| label: fig-identify-stationary-time-series
#| fig-cap: "Which of these series are stationary? (a) Google closing stock price in 2015; (b) Daily change in the Google stock price in 2015; (c) Annual number of strikes in the US; (d) Monthly sales of new one-family houses sold in the US; (e) Annual price of a dozen eggs in the US (constant dollars); (f) Monthly total of pigs slaughtered in Victoria, Australia; (g) Annual total of Canadian Lynx furs traded by the Hudson Bay Company; (h) Quarterly Australian beer production; (i) Monthly Australian gas production. "
a <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) == 2015) %>%
  autoplot(Close) +
  labs(x = "Day", y = "$US", title = "(a) Google closing price")

b <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) == 2015) %>%
  transmute(change = difference(Close)) %>%
  autoplot(change) +
  labs(x = "Day", y = "$US", title = "(b) Change in Google price")

c <- strikes %>%
  as_tsibble() %>%
  rename(Year = index, Count = value) %>%
  autoplot(Count) +
  labs(x = "Year", y = "Number of strikes", title = "(c) Strikes: US")

d <- hsales %>%
  as_tsibble() %>%
  rename(Month = index, Count = value) %>%
  autoplot(Count) +
  labs(x = "Month", y = "Number of houses", title = "(d) House sales: US")

e <- eggs %>%
  as_tsibble() %>%
  rename(Year = index, Price = value) %>%
  autoplot(Price) +
  labs(x = "Year", y = "$US (constant prices)", title = "(e) Egg prices: US")

f <- aus_livestock %>%
  filter(Animal == "Pigs", State == "Victoria") %>%
  autoplot(Count) +
  labs(x = "Month", y = "Number of pigs", title = "(f) Pigs slaughtered: Victoria, Australia")

g <- lynx %>%
  as_tsibble() %>%
  rename(Year = index, Count = value) %>%
  filter(Year > 1840) %>%
  autoplot(Count) +
  labs(x = "Year", y = "Number of lynx", title = "(g) Lynx trapped: Canada")

h <- aus_production %>%
  filter(year(Quarter) %in% c(1991:1995)) %>%
  autoplot(Beer) +
  labs(x = "Quarter", y = "Megalitres", title = "(h) Beer production: Australia")

i <- aus_production %>%
  autoplot(Gas) +
  labs(x = "Quarter", y = "Petajoules", title = "(i) Gas production: Australia")

(a + b + c) / (d + e + f) / (g + h + i)
```

Consider the nine series plotted in @fig-identify-stationary-time-series. </br>
Which of these do you think are stationary? </br>
Obvious seasonality rules out series (d), (h) and (i). </br>
Trends and changing levels rules out series (a), (c), (e), (f) and (i). </br>
Increasing variance also rules out (i). </br>
That leaves only (b) and (g) as stationary series.

At first glance, the strong cycles in series (g)
    might appear to make it non-stationary. </br>
But these cycles are aperiodic —
    they are caused when the lynx population becomes too large for the available feed,
    so that they stop breeding and the population falls to low numbers,
    then the regeneration of their food sources allows the population to grow again,
    and so on. </br>
In the long-term, the timing of these cycles is not predictable. </br>
Hence the series is stationary.


### Differencing {sec-differencing}

In @fig-identify-stationary-time-series,
    note that the Google stock price was non-stationary in panel (a),
    but the daily changes were stationary in panel (b).

> **differencing** </br>
    compute the differences between consecutive observations.

This shows one way to make a non-stationary time series stationary.

Transformations
    such as logarithms can help to stabilise the variance of a time series. </br>
Differencing
    can help stabilise the mean of a time series
    by removing changes in the level of a time series,
    and therefore eliminating (or reducing) trend and seasonality.

As well as the time plot of the data,
    the ACF plot is also useful for identifying non-stationary time series. </br>
For a stationary time series,
    the ACF will drop to zero relatively quickly,
    while the ACF of non-stationary data decreases slowly. </br>
Also, for non-stationary data, the value of $r_1$ is often large and positive.

```{r}
#| label: fig-google-closing-price-acf
#| fig-cap: "The ACF of the Google closing stock price in 2015 (left) and of the daily changes in Google closing stock price in 2015 (right)."
p1 <- google_2015 %>%
  ACF(Close) %>%
  autoplot() +
  labs(title = "Google closing stock price")

p2 <- google_2015 %>%
  mutate(diff_close = difference(Close)) %>%
  ACF(diff_close) %>%
  autoplot() +
  labs(title = "Changes in Google closing price")

p1 + p2


google_2015 %>%
  mutate(diff_close = difference(Close)) %>%
  features(diff_close, ljung_box, lag = 10)
```

The ACF of the differenced Google stock price
    looks just like that of a white noise series. </br>
Only one autocorrelation is outside of the 95% limits,
    and the Ljung-Box $Q^∗$ statistic has a p-value of 0.637 (for $h=10$). </br>
This suggests that the daily change in the Google stock price
    is essentially a random amount
    which is uncorrelated with that of previous days.


### Random walk model {#sec-random-walk-model}

> The **differenced series** </br>
    is the change between consecutive observations in the original series,
    and can be written as
    $y'_t = y_t - y_{t-1}.$ </br>

The differenced series will have only $T−1$ values,
    since it is not possible to calculate a difference $y'_1$
    for the first observation.

When the differenced series is white noise,
    the model for the original series can be written as
    $y_t - y_{t-1} = \varepsilon_t,$
where
    $\varepsilon_t$ denotes white noise. </br>
Rearranging this leads to the **random walk** model
    $y_t = y_{t-1} + \varepsilon_t.$

Random walk models are widely used for non-stationary data,
    particularly financial and economic data. </br>
Random walks typically have:
- long periods of apparent trends up or down
- sudden and unpredictable changes in direction.

The forecasts from a random walk model are equal to the last observation,
    as future movements are unpredictable,
    and are equally likely to be up or down. </br>
Thus, the random walk model underpins naïve forecasts,
    first introduced in @sec-simple-forecasting-methods (Chapter 5).

A closely related model allows the differences to have a non-zero mean. </br>
Then
$y_t - y_{t-1} = c + \varepsilon_t \quad \text{or} \quad {y_t = c + y_{t-1} + \varepsilon_t}.$ </br>
The value of $c$
    is the average of the changes between consecutive observations. </br>
If $c$ is positive,
    then the average change is an increase in the value of $y_t$. </br>
    Thus, $y_t$ will tend to drift upwards. </br>
However, if $c$ is negative,
    $y_t$ will tend to drift downwards.

This is the model behind the **Drift method**,
    also discussed in @sec-simple-forecasting-methods (Chapter 5).


### Second-order differencing {#sec-second-order-differencing}

Occasionally the differenced data will not appear to be stationary
    and it may be necessary to difference the data a second time
    to obtain a stationary series:
$$
\begin{align*}
    y''_{t}
        &= y'_{t} - y'_{t - 1} \\
        &= (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) \\
        &= y_t - 2y_{t-1} + y_{t-2}.
\end{align*}
$$
In this case, $y''_{t}$ will have $T−2$ values. </br>
Then, we would model the “change in the changes” of the original data. </br>
In practice, it is almost never necessary to go beyond second-order differences.


### Seasonal differencing {#sec-seasonal-differencing}

> A **seasonal difference** </br>
    is the difference between an observation and the previous observation
    from the same season.

So
    $y'_t = y_t - y_{t-m},$
where
    $m$ = the number of seasons. </br>
These are also called **lag-m differences**,
    as we subtract the observation after a lag of $m$ periods.

If seasonally differenced data appear to be white noise,
    then an appropriate model for the original data is
    $y_t = y_{t−m} + \varepsilon_t$.

Forecasts from this model are equal to
    the last observation from the relevant season. </br>
That is, this model gives seasonal naïve forecasts,
    introduced in @sec-simple-forecasting-methods (Chapter 5).

The bottom panel in @fig-a10-sales-data-log-seasonal-difference
    shows the seasonal differences of the logarithm of the monthly scripts
    for A10 (antidiabetic) drugs sold in Australia. </br>
The transformation and differencing have made the series look relatively stationary.

```{r}
#| label: fig-a10-sales-data-log-seasonal-difference
#| fig-cap: "Logs and seasonal differences of the A10 (antidiabetic) sales data. The logarithms stabilise the variance, while the seasonal differences remove the seasonality and trend."
PBS %>%
  filter(ATC2 == "A10") %>%
  summarise(Cost = sum(Cost)/1e6) %>%
  transmute(
    `Sales ($million)` = Cost,
    `Log sales` = log(Cost),
    `Annual change in log sales` = difference(log(Cost), 12)
  ) %>%
  pivot_longer(-Month, names_to = "Type", values_to = "Sales") %>%
  mutate(
    Type = factor(Type, levels = c(
      "Sales ($million)",
      "Log sales",
      "Annual change in log sales"
      )
    )
  ) %>%
  ggplot(aes(x = Month, y = Sales)) +
  geom_line() +
  facet_grid(vars(Type), scales = "free_y") +
  labs(title = "Antidiabetic drug sales", y = NULL)
```

To distinguish seasonal differences from ordinary differences,
    we sometimes refer to ordinary differences as **first differences**,
        meaning differences at lag 1.

Sometimes it is necessary to take both
    a seasonal difference and a first difference
    to obtain stationary data. </br>
@fig-corticosteroid-drug-sales-transforming-differencing
    plots Australian corticosteroid drug sales ($AUD) (top panel). </br>
Here, the data are first transformed using logarithms (second panel),
    then seasonal differences are calculated (third panel). </br>
The data still seem somewhat non-stationary,
    and so a further lot of first differences are computed (bottom panel).

```{r}
#| label: fig-corticosteroid-drug-sales-transforming-differencing
#| fig-cap: "Top panel: Corticosteroid drug sales ($AUD). Other panels show the same data after transforming and differencing."
PBS %>%
  filter(ATC2 == "H02") %>%
  summarise(Cost = sum(Cost)/1e6) %>%
  transmute(
    `Sales ($million)` = Cost,
    `Log sales` = log(Cost),
    `Annual change in log sales` = difference(log(Cost), 12),
    `Doubly differenced log sales` = difference(difference(log(Cost), 12), 1)
  ) %>%
  pivot_longer(-Month, names_to = "Type", values_to = "Sales") %>%
  mutate(
    Type = factor(Type, levels = c(
      "Sales ($million)",
      "Log sales",
      "Annual change in log sales",
      "Doubly differenced log sales"
      )
    )
  ) %>%
  ggplot(aes(x = Month, y = Sales)) +
  geom_line() +
  facet_grid(vars(Type), scales = "free_y") +
  labs(title = "Corticosteroid drug sales", y = NULL)
```

There is a degree of subjectivity in selecting which differences to apply. </br>
The seasonally differenced data in @fig-a10-sales-data-log-seasonal-difference
    do not show substantially different behaviour
    from the seasonally differenced data in
    @fig-corticosteroid-drug-sales-transforming-differencing. </br>
In the latter case,
    we could have decided to stop with the seasonally differenced data,
    and not done an extra round of differencing. </br>
In the former case,
    we could have decided that the data were not sufficiently stationary
    and taken an extra round of differencing. </br>
Some formal tests for differencing are discussed below,
    but there are always some choices to be made in the modelling process,
    and different analysts may make different choices.

If
    $y'_t = y_t - y_{t-m}$
    denotes a seasonally differenced series,
    then the twice-differenced series is
$$
\begin{align*}
    y''_t
        &= y'_t - y'_{t-1} \\
        &= (y_t - y_{t-m}) - (y_{t-1} - y_{t-m-1}) \\
        &= y_t -y_{t-1} - y_{t-m} + y_{t-m-1}.
\end{align*}
$$

When both seasonal and first differences are applied,
    it makes no difference which is done first—the result will be the same. </br>
However, if the data have a strong seasonal pattern,
    we recommend that seasonal differencing be done first,
    because the resulting series will sometimes be stationary
    and there will be no need for a further first difference. </br>
If first differencing is done first,
    there will still be seasonality present.

Beware that applying more differences than required
    will induce false dynamics or autocorrelations
    that do not really exist in the time series. </br>
Therefore, do as few differences as necessary to obtain a stationary series.

It is important that if differencing is used,
    the differences are interpretable. </br>
First differences
    are the change between one observation and the next. </br>
Seasonal differences
    are the change between one year to the next. </br>
Other lags
    are unlikely to make much interpretable sense and should be avoided.


### Unit root tests {#sec-unit-root-tests}

One way to determine more objectively whether differencing is required
    is to use a **unit root test**. </br>
These are statistical hypothesis tests of stationarity
    that are designed for determining whether differencing is required.

A number of unit root tests are available,
    which are based on different assumptions
    and may lead to conflicting answers. </br>
In our analysis, we use the
    **Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test** (Kwiatkowski et al., 1992). </br>
In this test,
    the null hypothesis is that the data are stationary,
    and we look for evidence that the null hypothesis is false. </br>
Consequently, small p-values (e.g., less than 0.05)
    suggest that differencing is required. </br>
`unitroot_kpss()`
    to compute the test

For **example**,
    let us apply it to the Google stock price data.
```{r}
google_2015 %>%
  features(Close, unitroot_kpss)
# alternative
unitroot_kpss(google_2015$Close)
```
The p-value is reported as 0.01
    if it is less than 0.01,
    and as 0.1
        if it is greater than 0.1. </br>
In this case, the test statistic (3.56) is bigger than the 1% critical value,
    so the p-value is less than 0.01,
    indicating that the null hypothesis is rejected. </br>
That is, the data are not stationary. </br>
We can difference the data, and apply the test again.

```{r}
google_2015 %>%
  mutate(diff_close = difference(Close)) %>%
  features(diff_close, unitroot_kpss)
```
This time, the test statistic is tiny,
    and well within the range we would expect for stationary data,
    so the p-value is greater than 0.1. </br>
We can conclude that the differenced data appear stationary.

`unitroot_ndiffs()`
    This process of using a sequence of KPSS tests
    to determine the appropriate number of first differences is carried out. </br>
```{r}
google_2015 %>%
  features(Close, unitroot_ndiffs)
```
As we saw from the KPSS tests above,
    one difference is required to make the `google_2015` data stationary.

`unitroot_nsdiffs()`
    A similar feature for determining whether seasonal differencing is required,
    which uses the measure of seasonal strength
    introduced in @sec-stl-features (Chapter 4)
    to determine the appropriate number of seasonal differences required. </br>
No seasonal differences are suggested if $F_S < 0.64$,
    otherwise one seasonal difference is suggested.

We can apply `unitroot_nsdiffs()` to the monthly total Australian retail turnover.
```{r}
aus_total_retail <- aus_retail %>%
  summarise(Turnover = sum(Turnover))

aus_total_retail %>%
  mutate(log_turnover = log(Turnover)) %>%
  features(log_turnover, unitroot_nsdiffs)

aus_total_retail %>%
  mutate(log_turnover = difference(log(Turnover), 12)) %>%
  features(log_turnover, unitroot_ndiffs)
```
Because `unitroot_nsdiffs()` returns 1
    (indicating one seasonal difference is required),
    we apply the `unitroot_ndiffs()` function
    to the seasonally differenced data. </br>
These functions suggest we should do both
    a seasonal difference and a first difference.


## Backshift notation {#sec-backshift-notation}

The **backward shift operator** $B$
    is a useful notational device when working with time series lags:
    $B y_{t} = y_{t-1}$. </br>
(Some references use
    $L$ for **lag**
    instead of $B$ for **backshift**.) </br>
In other words, $B$, operating on $y_t$,
    has the effect of shifting the data back one period. </br>
Two applications of $B$ to $y_t$
    shifts the data back two periods:
    $B(B y_{t}) = B^{2}y_{t} = y_{t-2}$. </br>
For monthly data,
    if we wish to consider “the same month last year,”
    the notation is
    $B^{12} y_{t} = y_{t-12}$.

The backward shift operator
    is convenient for describing the process of *differencing*. </br>
A first difference can be written as
    $y'_{t} = y_{t} - y_{t-1} = y_t - B y_{t} = (1 - B) y_{t}$. </br>
So a first difference can be represented by $(1 - B)$. </br>
Similarly, if second-order differences have to be computed, then:
    $y''_{t} = y_{t} - 2y_{t - 1} + y_{t - 2} = (1 -2B + B^2)y_t = (1 - B)^{2} y_{t}$. </br>
In general, a $d$ th-order difference can be written as
$(1 - B)^{d} y_{t}$.

Backshift notation is particularly useful when combining differences,
    as the operator can be treated using ordinary algebraic rules. </br>
In particular, terms involving $B$ can be multiplied together.

For **example**,
    a seasonal difference followed by a first difference can be written as
$$
\begin{align*}
    (1 - B)(1 - B^m)y_t
        &= (1 - B - B^m + B^{m+1})y_t \\
        &= y_t - y_{t-1} - y_{t-m} + y_{t-m-1},
\end{align*}
$$
the same result we obtained earlier.


## Autoregressive models {#sec-autoregressive-models}

In a **multiple regression model**,
    introduced in [Chapter 7](./chapter_7__Time_series_regression_model.qmd),
    we forecast the variable of interest
    using a linear combination of predictors. </br>
In an **autoregression model**,
    we forecast the variable of interest
    using a linear combination of past values of the variable. </br>
The term autoregression indicates
    that it is a regression of the variable against itself.

Thus, an autoregressive model of order $p$ can be written as
    $y_{t} = c + \phi_{1} y_{t-1} + \phi_{2} y_{t-2} + \dots + \phi_{p} y_{t-p} + \varepsilon_{t},$
where
    $\varepsilon_{t}$ is white noise. </br>
This is like a multiple regression
    but with lagged values of $y_t$ as predictors. </br>
We refer to this as an **$AR(p)$** model,
    an autoregressive model of order $p$.

Autoregressive models are remarkably flexible
    at handling a wide range of different time series patterns. </br>
Changing the parameters $\phi_1, \dots, \phi_p$
    results in different time series patterns. </br>
The variance of the error term $\varepsilon_t$
    will only change the scale of the series,
    not the patterns.

For an $AR(1)$ model:
    when $\phi_1 = 0$ and $c = 0$,
        $y_t$ is equivalent to white noise;
    when $\phi_1 = 1$ and $c = 0$,
        $y_t$ is equivalent to a random walk;
    when $\phi_1 = 1$ and $c \ne 0$,
        $y_t$ is equivalent to a random walk with drift;
    when $\phi_1 < 0$,
        $y_t$ tends to oscillate around the mean.

We normally restrict autoregressive models to stationary data,
    in which case some constraints on the values of the parameters are required. </br>
For an $AR(1)$ model:
    $-1 < \phi_1 < 1$. </br>
For an $AR(2)$ model:
    $-1 < \phi_2 < 1$, $\phi_1 + \phi_2 < 1$, $\phi_2 - \phi_1 < 1$.

When $p \ge 3$,
    the restrictions are much more complicated. </br>
The `fable` package takes care of these restrictions when estimating a model.


## Moving average models {#sec-moving-average-models}

Rather than using past values of the forecast variable in a regression,
    a moving average model uses past forecast errors in a regression-like model,
$$
y_{t} = c + \varepsilon_t
    + \theta_{1} \varepsilon_{t-1} + \theta_{2} \varepsilon_{t-2}
    + \dots + \theta_{q} \varepsilon_{t-q},
$$
where
    $\varepsilon_t$ is white noise.
We refer to this as an **$MA(q)$** model,
    a moving average model of order $q$. </br>
Of course, we do not observe the values of $\varepsilon_t$,
    so it is not really a regression in the usual sense.

each value of $y_t$
    can be thought of as a weighted moving average of the past few forecast errors
    (although the coefficients will not normally sum to one). </br>
However, moving average models should not be confused with
    the moving average smoothing we discussed in
    [Chapter 3](./chapter_3_time_series_decomposition.qmd). </br>
A **moving average model**
    is used for forecasting future values, while
    **moving average smoothing**
        is used for estimating the trend-cycle of past values.

Changing the parameters $\theta_1, \dots, \theta_q$
    results in different time series patterns. </br>
As with autoregressive models,
    the variance of the error term $\varepsilon_t$
    will only change the scale of the series,
    not the patterns.

It is possible to write any stationary $AR(p)$ model as an $MA(\infty)$ model. </br>
**example**,
    using repeated substitution, we can demonstrate this for an $AR(1)$ model:
$$
\begin{align*}
    y_t
        &= \phi_1 y_{t-1} + \varepsilon_t \\
        &= \phi_1(\phi_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \\
        &= \phi_1^2 y_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t \\
        &= \phi_1^3 y_{t-3} + \phi_1^2 \varepsilon_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t \\
        & \text{etc.}
\end{align*}
$$
Provided $-1 < \phi_1 < 1$,
    the value of $\phi_1^k$ will get smaller as $k$ gets larger. </br>
So eventually we obtain
    $y_t = \varepsilon_t + \phi_1 \varepsilon_{t-1} + \phi_1^2 \varepsilon_{t-2} + \phi_1^3 \varepsilon_{t-3} + \cdots,$
    an $MA(\infty)$ process.

The reverse result holds
    if we impose some constraints on the MA parameters. </br>
Then the MA model is called **invertible**. </br>
That is, we can write any invertible $MA(q)$ process as an $AR(\infty)$ process.

Invertible models are not simply introduced
    to enable us to convert from MA models to AR models. </br>
They also have some desirable mathematical properties.

**example**,
    consider the MA(1) process,
    $y_{t} = \varepsilon_t + \theta_{1} \varepsilon_{t-1}$. </br>
In its AR($\infty$) representation,
    the most recent error can be written as
    a linear function of current and past observations:
    $\varepsilon_t = \sum_{j=0}^\infty (-\theta_1)^j y_{t-j}$. </br>
When $|\theta_1| > 1$,
    the weights increase as lags increase,
    so the more distant the observations
    the greater their influence on the current error. </br>
When $|\theta_1| = 1$,
    the weights are constant in size,
    and the distant observations
    have the same influence as the recent observations. </br>
As neither of these situations make much sense,
    we require $|\theta_1| < 1$,
    so the most recent observations have higher weight
    than observations from the more distant past. </br>
Thus, the process is invertible when $|\theta_1| < 1$.

The invertibility constraints for other models
    are similar to the stationarity constraints.
- For an MA(1) model:
    $−1 < \theta_1 < 1$.
- For an MA(2) model:
    $-1 < \theta_2 < 1,~ \theta_2 + \theta_1 >-1,~ \theta_1 -\theta_2 < 1$.

More complicated conditions hold for $q \ge 3$. </br>
Again, the `fable` package will take care of these constraints
    when estimating the models.


## Non-seasonal ARIMA models {#sec-non-seasonal-ARIMA-models}

If we combine differencing with autoregression and a moving average model,
    we obtain a **non-seasonal ARIMA model**.

**ARIMA** is an acronym for
    **AutoRegressive Integrated Moving Average**
    (in this context, **integration**
        is the reverse of differencing).

The full model can be written as
$$
y'_{t} =
    c
    + \phi_{1} y'_{t-1} + \cdots + \phi_{p} y'_{t-p}
    + \theta_{1} \varepsilon_{t-1} + \cdots + \theta_{q} \varepsilon_{t-q}
    + \varepsilon_{t},
$$ {#eq-arima}
where
    $y'_{t}$ is the differenced series
    (it may have been differenced more than once). </br>
The “predictors” on the right hand side include both
    lagged values of $y_t$
    and lagged errors. </br>
We call this an **$ARIMA(p,d,q)$ model**,
where
    $p$ =  order of the autoregressive part;
    $d$ =  degree of first differencing involved;
    $q$ =  order of the moving average part.

The same stationarity and invertibility conditions
    that are used for autoregressive and moving average models
    also apply to an ARIMA model.

Many of the models we have already discussed
    are special cases of the ARIMA model,
    as shown in @tbl-special-cases-arima-models. </br>

|    |    |
| :- | :- |
| White noise | ARIMA(0,0,0) with no constant |
| Random walk | ARIMA(0,1,0) with no constant |
| Random walk with drift | ARIMA(0,1,0) with a constant |
| Autoregression | ARIMA($p$,0,0) |
| Moving average | ARIMA(0,0,$q$) |

: Special cases of ARIMA models {#tbl-special-cases-arima-models}


Once we start combining components in this way to form more complicated models,
    it is much easier to work with the backshift notation. </br>
**example**,
@eq-arima can be written in backshift notation as
$$
\begin{array}
    {c c c c}
    (1 - \phi_1 B - \cdots - \phi_p B^p)
    & (1 - B)^d y_{t}
        &= &c
        + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t \\
        {\uparrow} & {\uparrow} & & {\uparrow} \\
        \text{AR($p$)} & \text{$d$ differences} & & \text{MA($q$)} \\
\end{array}
$$

Selecting appropriate values for $p$, $d$ and $q$ can be difficult. </br>
However, the `ARIMA()` function from the fable package
    will do it for you automatically. </br>
In @sec-arima-modelling-in-fable,
    we will learn how this function works,
    along with some methods for choosing these values yourself.


### Egyptian exports {#sec-egyptian-exports}

@fig-egyptian-exports
    shows Egyptian exports as a percentage of GDP from 1960 to 2017.
```{r}
#| label: fig-egyptian-exports
#| fig-cap: "Annual Egyptian exports as a percentage of GDP since 1960."
global_economy %>%
  filter(Code == "EGY") %>%
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Egyptian Exports")
```

The following R code selects a non-seasonal ARIMA model automatically.
```{r}
fit <- global_economy %>%
  filter(Code == "EGY") %>%
  model(ARIMA(Exports))

report(fit)
```

This is an ARIMA(2,0,1) model:
$y_t = 2.56 + 1.68 y_{t-1} - 0.80 y_{t-2} -0.69 \varepsilon_{t-1} + \varepsilon_{t},$
where
    $\varepsilon_t$ is white noise
    with a standard deviation of $2.837 = \sqrt{8.046}$. </br>
Forecasts from the model are shown in @fig-egyptian-exports-forecasts. </br>
Notice how they have picked up the cycles
    evident in the Egyptian economy over the last few decades.
```{r}
#| label: fig-egyptian-exports-forecasts
#| fig-cap: "Forecasts of Egyptian exports."
fit %>% forecast(h=10) %>%
  autoplot(global_economy) +
  labs(y = "% of GDP", title = "Egyptian Exports")
```


### Understanding ARIMA models {#sec-understanding-arima-models}

The `ARIMA()` function is useful,
    but anything automated can be a little dangerous,
    and it is worth understanding something of the behaviour of the models
    even when you rely on an automatic procedure to choose the model for you.

The **constant** $c$
    has an important effect
    on the long-term forecasts obtained from these models. </br>
- If $c = 0$ and $d = 0$,
    the long-term forecasts will go to zero. </br>
- If $c = 0$ and $d = 1$,
    the long-term forecasts will go to a non-zero constant. </br>
- If $c = 0$ and $d = 2$,
    the long-term forecasts will follow a straight line. </br>
- If $c \ne 0$ and $d = 0$,
    the long-term forecasts will go to the mean of the data. </br>
- If $c \ne 0$ and $d = 1$,
    the long-term forecasts will follow a straight line. </br>
- If $c \ne 0$ and $d = 2$,
    the long-term forecasts will follow a quadratic trend.
    (This is not recommended, and `fable` will not permit it.)

The value of $d$
    also has an effect on the prediction intervals —
    the higher the value of $d$,
    the more rapidly the prediction intervals increase in size. </br>
For $d = 0$,
    the long-term forecast standard deviation
    will go to the standard deviation of the historical data,
    so the prediction intervals will all be essentially the same.

This behaviour is seen in @fig-egyptian-exports-forecasts
    where $d = 0$ and $c \ne 0$. </br>
In this figure,
    the prediction intervals are almost the same width
    for the last few forecast horizons,
    and the final point forecasts are close to the mean of the data.

The value of $p$ is important if the data show cycles. </br>
To obtain cyclic forecasts,
    it is necessary to have $p \ge 2$,
    along with some additional conditions on the parameters. </br>
For an AR(2) model, cyclic behaviour occurs if
    $\phi_1^2 + 4 \phi_2 < 0$
    (as is the case for the Egyptian Exports model). </br>
In that case, the average period of the cycles is
    $ \frac{2 \pi}{\text{arc cos}(-\phi_1(1 - \phi_2) / (4 \phi_2))}$. </br>
**arc cos**
    is the inverse cosine function.
    (also labelled $acos$ or $cos^{-1}$)


### ACF and PACF plots {#sec-acf-pacf-plots}

It is usually not possible to tell,
    simply from a time plot,
    what values of $p$ and $q$ are appropriate for the data. </br>
However, it is sometimes possible to use the ACF plot,
    and the closely related PACF plot,
    to determine appropriate values for $p$ and $q$.

Recall that an ACF plot shows
    the autocorrelations
    which measure the relationship between $y_t$ and $y_{t−k}$
    for different values of $k$. </br>
Now if $y_t$ and $y_{t−1}$ are correlated,
    then $y_{t−1}$ and $y_{t−2}$ must also be correlated. </br>
However, then $y_t$ and $y_{t−2}$ might be correlated,
    simply because they are both connected to $y_{t−1}$,
    rather than because of any new information contained in $y_{t−2}$
    that could be used in forecasting $y_t$.

To overcome this problem, we can use **partial autocorrelations**. </br>
These measure the relationship between $y_t$ and $y_{t−k}$
    after removing the effects of lags $1, 2, 3, \dots, k-1$. </br>
So the first partial autocorrelation is identical to the first autocorrelation,
    because there is nothing between them to remove. </br>
Each partial autocorrelation can be estimated
    as the last coefficient in an autoregressive model. </br>
Specifically, $\alpha_k$, the $k$ th partial autocorrelation coefficient,
    is equal to the estimate of
    $\phi_k$ in an AR($k$) model. </br>
In practice, there are more efficient algorithms for computing $\alpha_k$
    than fitting all of these autoregressions,
    but they give the same results.

@fig-egyptian-exports-acf and @fig-egyptian-exports-pacf
    shows the ACF and PACF plots for the Egyptian exports data
    shown in @fig-egyptian-exports. </br>
The partial autocorrelations
    have the same critical values of $\pm 1.96 / \sqrt{T}$
    as for ordinary autocorrelations,
    and these are typically shown on the plot as in Figure @fig-egyptian-exports-pacf.
```{r}
#| label: fig-egyptian-exports-acf
#| fig-cap: "ACF of Egyptian exports."
global_economy %>%
  filter(Code == "EGY") %>%
  ACF(Exports) %>%
  autoplot()


#| label: fig-egyptian-exports-pacf
#| fig-cap: "PACF of Egyptian exports."
global_economy %>%
  filter(Code == "EGY") %>%
  PACF(Exports) %>%
  autoplot()
```

A convenient way to produce a time plot, ACF plot and PACF plot in one command
    is to use the `gg_tsdisplay()` function with `plot_type = "partial"`.

If the data are from an ARIMA($p$,$d$,0) or ARIMA(0,$d$,$q$) model,
    then the ACF and PACF plots can be helpful
    in determining the value of $p$ or $q$. </br>
If $p$ and $q$ are both positive,
    then the plots do not help in finding suitable values of $p$ and $q$.

The data may follow an ARIMA($p$,$d$,0) model
    if the ACF and PACF plots of the differenced data show the following patterns:
- the ACF is exponentially decaying or sinusoidal;
- there is a significant spike at lag $p$ in the PACF, but none beyond lag $p$.

The data may follow an ARIMA(0,$d$,$q$) model
    if the ACF and PACF plots of the differenced data show the following patterns:
- the PACF is exponentially decaying or sinusoidal;
- there is a significant spike at lag $q$ in the ACF, but none beyond lag $q$.

In Figure @fig-egyptian-exports-acf,
    we see that there is a decaying sinusoidal pattern in the ACF,
    and in Figure @fig-egyptian-exports-pacf
        the PACF shows the last significant spike at lag 4. </br>
This is what you would expect from an ARIMA(4,0,0) model.

```{r}
fit2 <- global_economy %>%
  filter(Code == "EGY") %>%
  model(ARIMA(Exports ~ pdq(4,0,0)))

report(fit2)
```

This model is only slightly worse
    than the ARIMA(2,0,1) model identified by `ARIMA()`
    (with an AICc value of 294.70 compared to 294.29).

We can also specify particular values of `pdq()` that `ARIMA()` can search for. </br>
**example**,
    to find the best ARIMA model with $p \in \{1,2,3\}$, $q \in \{0,1,2\}$ and $d=1$,
    you could use
    `ARIMA(y ~ pdq(p=1:3, d=1, q=0:2))`.


## Estimation and order selection {#sec-estimation-order-selection}

### Maximum likelihood estimation {#sec-maximum-likelihood-estimation}

Once the model order has been identified
    (i.e., the values of $p$, $d$ and $q$),
    we need to estimate the parameters
    $c$, $\phi_1, \dots ,\phi_p$, $\theta_1, \dots ,\theta_q$.

When `fable` estimates the ARIMA model,
    it uses **maximum likelihood estimation (MLE)**. </br>

> **Maximum Likelihood Estimation (MLE)** </br>
This technique
    finds the values of the parameters
    which maximise the probability of obtaining the data that we have observed.

For ARIMA models,
    MLE is similar to the least squares estimates
    that would be obtained by minimising
    $\sum_{t=1}^T \varepsilon_t^2$.
    (For the regression models considered in
    [Chapter 7](./chapter_7__Time_series_regression_model.qmd),
    MLE gives exactly the same parameter estimates
    as least squares estimation.)  </br>
Note that ARIMA models are much more complicated to estimate than regression models,
    and different software will give slightly different answers
    as they use different methods of estimation,
    and different optimisation algorithms.

In practice, the `fable` package will report
    the value of the **log likelihood** of the data;
    that is, the logarithm of the probability of
        the observed data coming from the estimated model. </br>
For given values of $p$, $d$ and $q$,
    `ARIMA()` will try to maximise the log likelihood when finding parameter estimates.


### Information Criteria {#sec-information-criteria}

**Akaike’s Information Criterion (AIC)**,
    which was useful in selecting predictors for regression
    (see @sec-selecting-predictors (Chapter 7)),
    is also useful for determining the order of an ARIMA model. </br>
It can be written as
    $\text{AIC} = -2 \log(L) + 2(p+q+k+1),$
where
    $L$ is the likelihood of the data,
    $k = 1$ if $c \ne 0$ and
    $k = 0$ if $c = 0$. </br>
Note that the last term in parentheses
    is the number of parameters in the model
    (including $\sigma^2$, the variance of the residuals).

For ARIMA models,
    the corrected AIC can be written as
    $\text{AICc} = \text{AIC} + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2},$
and
    the Bayesian Information Criterion can be written as
    $\text{BIC} = \text{AIC} + [\log(T)-2](p+q+k+1).$

Good models are obtained by minimising the AIC, AICc or BIC. </br>
Our preference is to use the AICc.

It is important to note that
    these information criteria tend not to be good guides to selecting
    the appropriate order of differencing $d$ of a model,
    but only for selecting the values of $p$ and $q$. </br>
This is because the differencing changes the data
    on which the likelihood is computed,
    making the AIC values between models with different orders of differencing
    not comparable. </br>
So we need to use some other approach to choose $d$,
    and then we can use the AICc to select $p$ and $q$.


## ARIMA modelling in fable {#sec-arima-modelling-in-fable}

### How does ARIMA() work? {#sec-how-does-arima-work}

The `ARIMA()` function in the `fable` package
    uses a variation of the Hyndman-Khandakar algorithm (Hyndman & Khandakar, 2008),
    which combines unit root tests, minimisation of the AICc and MLE
    to obtain an ARIMA model. </br>
The arguments to `ARIMA()` provide for many variations on the algorithm. </br>
What is described here is the default behaviour.

Hyndman-Khandakar algorithm for automatic ARIMA modelling
1. The number of differences $0 \le d\le 2$ is determined using repeated KPSS tests.
2. The values of $p$ and $q$ are then chosen by minimising the AICc
    after differencing the data $d$ times.
    Rather than considering every possible combination of $p$ and $q$,
        the algorithm uses a stepwise search to traverse the model space.
    
    a. Four initial models are fitted:
    - ARIMA $(0,d,0)$,
    - ARIMA $(2,d,2)$,
    - ARIMA $(1,d,0)$,
    - ARIMA $(0,d,1)$.
    
    A constant is included unless $d = 2$.
    If $d \le 1$, an additional model is also fitted:
    - ARIMA $(0,d,0)$ without a constant.
    
    b. The best model (with the smallest AICc value) fitted in step (a)
        is set to be the “current model.”
    
    c. Variations on the current model are considered:
    - vary $p$ and/or $q$ from the current model by $\pm 1$;
    - include/exclude $c$ from the current model.

    The best model considered so far
        (either the current model or one of these variations)
        becomes the new current model.
    
    d. Repeat Step 2(c) until no lower AICc can be found.

![An illustrative example of the Hyndman-Khandakar stepwise search process](./arma_grid_search.png):
    An illustrative example of the Hyndman-Khandakar stepwise search process
    illustrates diagrammatically how the Hyndman-Khandakar algorithm
    traverses the space of the ARMA orders,
    through an example. </br>
The grid covers combinations of ARMA $(p,q)$ orders
    starting from the top-left corner with an ARMA $(0,0)$,
    with the AR order increasing down the vertical axis,
    and the MA order increasing across the horizontal axis.

The orange cells show the initial set of models considered by the algorithm. </br>
In this example, the ARMA(2,2) model
    has the lowest AICc value amongst these models. </br>
This is called the **current model**
    and is shown by the black circle. </br>
The algorithm then searches over neighbouring models
    as shown by the blue arrows. </br>
If a better model is found then this becomes the new **current model**. </br>
In this **example**,
    the new “current model” is the ARMA(3,3) model. </br>
The algorithm continues in this fashion until no better model can be found. </br>
In this **example**
    the model returned is an ARMA(4,2) model.

The default procedure
    will switch to a new “current model”
    as soon as a better model is identified,
    without going through all the neighbouring models. </br>
The full neighbourhood search is done when `greedy=FALSE`.

The default procedure also uses some approximations to speed up the search. </br>
These approximations can be avoided with the argument `approximation=FALSE`. </br>
It is possible that the minimum AICc model
    will not be found due to these approximations,
    or because of the use of the stepwise procedure. </br>
A much larger set of models will be searched
  if the argument `stepwise=FALSE` is used. </br>
See the help file for a full description of the arguments.


### Modelling procedure {#sec-modelling-procedure}

When fitting an ARIMA model to a set of (non-seasonal) time series data,
    the following procedure provides a useful general approach.

1. Plot the data and identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.
3. If the data are non-stationary, take first differences of the data until the data are stationary.
4. Examine the ACF/PACF: Is an ARIMA $(p,d,0)$ or ARIMA $(0,d,q)$ model appropriate?
5. Try your chosen model(s), and use the AICc to search for a better model.
6. Check the residuals from your chosen model by
    plotting the ACF of the residuals,
    and doing a portmanteau test of the residuals.
    If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.

The Hyndman-Khandakar algorithm only takes care of steps 3–5. </br>
So even if you use it,
    you will still need to take care of the other steps yourself.

The process is summarised in
    ![General process for forecasting using an ARIMA model](./arima_flowchart.png).



### Portmanteau tests of residuals for ARIMA models {#sec-rortmanteau-tests-arima-models-residuals}

With ARIMA models, more accurate portmanteau tests are obtained
    if the degrees of freedom of the test statistic are adjusted
    to take account of the number of parameters in the model. </br>
Specifically, we use $\ell - K$ degrees of freedom in the test,
    where $K$ is the number of AR and MA parameters in the model. </br>
So for the non-seasonal models that we have considered so far, $K = p + q$. </br>
The value of $K$ is passed to the `ljung_box` function via the argument `dof`,
    as shown in the example below.


### Example: Central African Republic exports {#sec-eg-central-african-republic-exports}

We will apply this procedure to the exports of the Central African Republic
```{r}
#| label: fig-central-african-republic-exports
#| fig-cap: "Exports of the Central African Republic as a percentage of GDP."
global_economy %>%
  filter(Code == "CAF") %>%
  autoplot(Exports) +
  labs(title = "Central African Republic exports", y="% of GDP")
```

1. The time plot shows some non-stationarity, with an overall decline.
    The improvement in 1994 was due to
        a new government which overthrew the military junta
        and had some initial success,
        before unrest caused further economic decline.
2. There is no evidence of changing variance,
    so we will not do a Box-Cox transformation.
3. To address the non-stationarity, we will take a first difference of the data.
```{r}
#| label: fig-central-african-republic-exports-acf-pacf
#| fig-cap: "Time plot and ACF and PACF plots for the differenced Central African Republic Exports."
global_economy %>%
  filter(Code == "CAF") %>%
  gg_tsdisplay(difference(Exports), plot_type = 'partial')
```

These now appear to be stationary.

4. The PACF shown in @fig-central-african-republic-exports-acf-pacf
    is suggestive of an AR(2) model;
    so an initial candidate model is an ARIMA(2,1,0).
    The ACF suggests an MA(3) model;
        so an alternative candidate is an ARIMA(0,1,3).
5. We fit both an ARIMA(2,1,0) and an ARIMA(0,1,3) model
    along with two automated model selections,
    one using the default stepwise procedure,
    and one working harder to search a larger model space.
```{r}
caf_fit <- global_economy %>%
  filter(Code == "CAF") %>%
  model(
    arima210 = ARIMA(Exports ~ pdq(2,1,0)),
    arima013 = ARIMA(Exports ~ pdq(0,1,3)),
    stepwise = ARIMA(Exports),
    search = ARIMA(Exports, stepwise=FALSE)
  )

caf_fit %>% pivot_longer(!Country, names_to = "Model name", values_to = "Orders")

glance(caf_fit) %>% arrange(AICc) %>% select(.model:BIC)
```

The four models have almost identical AICc values. </br>
Of the models fitted,
    the full search has found that an ARIMA(3,1,0) gives the lowest AICc value,
    closely followed by the ARIMA(2,1,0) and ARIMA(0,1,3) —
    the latter two being the models that we guessed
    from the ACF and PACF plots. </br>
The automated stepwise selection has identified an ARIMA(2,1,2) model,
    which has the highest AICc value of the four models.

6. The ACF plot of the residuals from the ARIMA(3,1,0) model shows that
    all autocorrelations are within the threshold limits,
    indicating that the residuals are behaving like white noise.
```{r}
#| label: fig-central-african-republic-exports-residual-plots
#| fig-cap: "Residual plots for the ARIMA(3,1,0) model."
caf_fit %>%#| 
  select(search) %>%
  gg_tsresiduals()
```

A portmanteau test (setting $K = 3$) returns a large p-value,
    also suggesting that the residuals are white noise.
```{r}
augment(caf_fit) %>%
  filter(.model == 'search') %>%
  features(.innov, ljung_box, lag = 10, dof = 3)
```

7. Forecasts from the chosen model are shown in
    @fig-central-african-republic-exports-forecasts
```{r}
#| label: fig-central-african-republic-exports-forecasts
#| fig-cap: "Forecasts for the Central African Republic Exports."
caf_fit %>%
  forecast(h = 5) %>%
  filter(.model == 'search') %>%
  autoplot(global_economy)
```

Note that the mean forecasts look very similar
    to what we would get with a random walk (equivalent to an ARIMA(0,1,0)). </br>
The extra work to include AR and MA terms
    has made little difference to the point forecasts in this example,
    although the prediction intervals are much narrower than for a random walk model.


### Understanding constants in R {#sec-understanding-constants-in-r}

A non-seasonal ARIMA model can be written as
$$
(1 - \phi_1B - \cdots - \phi_p B^p)(1 - B)^d y_t =
    c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t,
$$ {#eq-non-seasonal-arima}
or equivalently as
$$
(1 - \phi_1B - \cdots - \phi_p B^p)(1 - B)^d (y_t - \mu t^d/d!) =
    (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t,
$$ {#eq-non-seasonal-arima-alternate}
where
    $c = \mu(1 - \phi_1 - \cdots - \phi_p)$ and
    $\mu$ is the mean of $(1-B)^d y_t$. </br>
The `fable` package
    uses the parameterisation of @eq-non-seasonal-arima
    and most other R implementations use @eq-non-seasonal-arima-alternate.

Thus, the inclusion of a constant in a non-stationary ARIMA model is equivalent to
    inducing a polynomial trend of order $d$ in the forecasts.
    (If the constant is omitted,
        the forecasts include a polynomial trend of order $d−1$.) </br>
When $d=0$,
    we have the special case that $\mu$ is the mean of $y_t$.

By default, the `ARIMA()` function
    will automatically determine if a constant should be included. </br>
For $d = 0$ or $d = 1$,
    a constant will be included if it improves the AICc value. </br>
If $d > 1$
    the constant is always omitted
    as a quadratic or higher order trend is particularly dangerous when forecasting.

The constant can be specified by including 0 or 1 in the model formula
    (like the intercept in `lm()`). </br>
**example**,
    to automatically select an ARIMA model with a constant, you could use
    `ARIMA(y ~ 1 + ...)`. </br>
Similarly, a constant can be excluded with `ARIMA(y ~ 0 + ...)`.


### Plotting the characteristic roots {#sec-plotting-characteristic-roots}

*(This is a more advanced section and can be skipped if desired.)*

We can re-write @eq-non-seasonal-arima as
    $\phi(B) (1-B)^d y_t = c + \theta(B) \varepsilon_t$
where
    $\phi(B) = (1 - \phi_1 B - \cdots - \phi_p B^p)$
        is a $p$ th order polynomial in $B$ and
    $\theta(B) = (1 + \theta_1 B + \cdots + \theta_q B^q)$
        is a $q$ th order polynomial in $B$.

The stationarity conditions for the model are that
    the $p$ complex roots of $\phi(B)$ lie outside the unit circle, and
    the invertibility conditions are that
        the $q$ complex roots of $\theta(B)$ lie outside the unit circle. </br>
So we can see whether the model is close to invertibility or stationarity
    by a plot of the roots in relation to the complex unit circle.

It is easier to plot the inverse roots instead,
    as they should all lie *within* the unit circle. </br>
This is easily done in R. </br>
For the ARIMA(3,1,0) model fitted to the Central African Republic Exports,
    we obtain @fig-central-african-republic-exports-inverse-characteristic-roots.
```{r}
#| label: fig-central-african-republic-exports-inverse-characteristic-roots
#| fig-cap: "Inverse characteristic roots for the ARIMA(3,1,0) model fitted to the Central African Republic Exports."
gg_arma(caf_fit %>% select(Country, search))
```

The three orange dots in the plot
    correspond to the roots of the polynomials $\phi(B)$. </br>
They are all inside the unit circle, as we would expect
    because `fable` ensures the fitted model is both stationary and invertible. </br>
Any roots close to the unit circle may be numerically unstable,
    and the corresponding model will not be good for forecasting.

The `ARIMA()` function
    will never return a model with inverse roots outside the unit circle. </br>
Models automatically selected by the `ARIMA()` function
    will not contain roots close to the unit circle either. </br>
Consequently, it is sometimes possible to find a model
    with better AICc value than `ARIMA()` will return,
    but such models will be potentially problematic.


## Forecasting {#sec-forecasting}

### Point forecasts {#sec-point-forecasts}

Although we have calculated forecasts from the ARIMA models in our examples,
    we have not yet explained how they are obtained. </br>
Point forecasts can be calculated using the following three steps.
1. Expand the ARIMA equation so that
    $y_t$ is on the left hand side
    and all other terms are on the right.
2. Rewrite the equation by replacing $t$ with $T+h$.
3. On the right hand side of the equation,
    replace future observations with their forecasts,
    future errors with zero, and
    past errors with the corresponding residuals.

Beginning with $h=1$,
    these steps are then repeated for $h=2,3,\dots$
    until all forecasts have been calculated.

The procedure is most easily understood via an example. </br>
We will illustrate it using a ARIMA(3,1,1) model which can be written as follows:
$$
(1 - \hat{\phi}_1B - \hat{\phi}_2 B^2 - \hat{\phi}_3 B^3)(1-B) y_t
    = (1 + \hat{\theta}_1 B)\varepsilon_{t}.
$$
Then we expand the left hand side to obtain
$$
\left[
    1 -(1 + \hat{\phi}_1)B +(\hat{\phi}_1 - \hat{\phi}_2)B^2
    + (\hat{\phi}_2 - \hat{\phi}_3)B^3 + \hat{\phi}_3 B^4
\right] y_t
    = (1 + \hat{\theta}_1 B)\varepsilon_{t},
$$
and applying the backshift operator gives
$$
y_t - (1 + \hat{\phi}_1)y_{t-1} + (\hat{\phi}_1 - \hat{\phi}_2)y_{t-2} 
    + (\hat{\phi}_2 - \hat{\phi}_3)y_{t-3} + \hat{\phi}_3 y_{t-4}
    = \varepsilon_t + \hat{\theta}_1 \varepsilon_{t-1}.
$$
Finally, we move all terms other than $y_t$ to the right hand side:
$$
y_t =
    (1 + \hat{\phi}_1)y_{t-1} - (\hat{\phi}_1 - \hat{\phi}_2)y_{t-2}
    - (\hat{\phi}_2 - \hat{\phi}_3)y_{t-3} -\hat{\phi}_3 y_{t-4}
    + \varepsilon_t + \hat{\theta}_1 \varepsilon_{t-1}.
$$ {#eq-arima311}

This completes the first step. </br>
While the equation now looks like an ARIMA(4,0,1),
    it is still the same ARIMA(3,1,1) model we started with. </br>
It cannot be considered an ARIMA(4,0,1)
    because the coefficients do not satisfy the stationarity conditions.

For the second step, we replace $t$ with $T+1$ in @eq-arima311:
$$
y_{T+1} =
    (1 + \hat{\phi}_1)y_{T} - (\hat{\phi}_1 - \hat{\phi}_2)y_{T-1}
    - (\hat{\phi}_2 - \hat{\phi}_3)y_{T-2} - \hat{\phi}_3 y_{T-3}
    + \varepsilon_{T+1} + \hat{\theta}_1 \varepsilon_{T}.
$$
Assuming we have observations up to time $T$,
    all values on the right hand side are known
    except for $\varepsilon_{T+1}$,
        which we replace with zero,
    and $\varepsilon_T$,
        which we replace with the last observed residual $e_T$:
$$
\hat{y}_{T+1|T} =
    (1 + \hat{\phi}_1)y_{T} - (\hat{\phi}_1 - \hat{\phi}_2)y_{T-1}
    - (\hat{\phi}_2 - \hat{\phi}_3)y_{T-2} - \hat{\phi}_3 y_{T-3}
    + \hat{\theta}_1 e_{T}.
$$

A forecast of $y_{T+2}$ is obtained by replacing $t$ with $T+2$ in @eq-arima311. </br>
All values on the right hand side will be known at time $T$
    except $y_{T+1}$
        which we replace with $\hat{y}_{T+1|T}$,
    and $\varepsilon_{T+2}$ and $\varepsilon_{T+1}$,
        both of which we replace with zero:
$$
\hat{y}_{T+2|T} =
    (1 + \hat{\phi}_1)\hat{y}_{T+1|T} - (\hat{\phi}_1 - \hat{\phi}_2)y_{T}
    - (\hat{\phi}_2 - \hat{\phi}_3)y_{T-1} - \hat{\phi}_3 y_{T-2}.
$$

The process continues in this manner for all future time periods. </br>
In this way, any number of point forecasts can be obtained.


### Prediction intervals {#sec-prediction-intervals}

The calculation of ARIMA prediction intervals is more difficult,
    and the details are largely beyond the scope of this book. </br>
We will only give some simple examples.

The first prediction interval is easy to calculate. </br>
If $\hat{\sigma}$ is the standard deviation of the residuals,
    then a 95% prediction interval is given by
    $\hat{y}_{T+1|T} \pm 1.96\hat{\sigma}$. </br>
This result is true for all ARIMA models regardless of their parameters and orders.

Multi-step prediction intervals for ARIMA(0,0,$q$) models
    are relatively easy to calculate. </br>
We can write the model as
$y_t = \varepsilon_t + \sum_{i=1}^q \theta_i \varepsilon_{t-i}.$ </br>
Then, the estimated forecast variance can be written as
$$
\hat\sigma_h^2 =
    \hat{\sigma}^2 \left[ 1 + \sum_{i=1}^{h-1} \hat{\theta}_i^2\right],
    \qquad\text{for $h=2,3,\dots$,}
$$
and a 95% prediction interval is given by
$\hat{y}_{T+h|T} \pm 1.96\hat\sigma_h$

In @sec-moving-average-models,
    we showed that an AR(1) model can be written as an MA($\infty$) model. </br>
Using this equivalence,
    the above result for MA($q$) models
    can also be used to obtain prediction intervals for AR(1) models.

More general results, and other special cases of
    multi-step prediction intervals for an ARIMA($p,d,q$) model,
    are given in more advanced textbooks such as Brockwell & Davis (2016).

The prediction intervals for ARIMA models are based on assumptions that
    the residuals are uncorrelated
    and normally distributed. </br>
If either of these assumptions does not hold,
    then the prediction intervals may be incorrect. </br>
For this reason, always plot the ACF and histogram of the residuals
    to check the assumptions before producing prediction intervals.

If the residuals are uncorrelated but not normally distributed,
    then bootstrapped intervals can be obtained instead,
    as discussed in @sec-distributional-forecasts-prediction-intervals. </br>
This is easily achieved
    by simply adding `bootstrap=TRUE` in the `forecast()` function.

In general, prediction intervals from ARIMA models increase
    as the forecast horizon increases. </br>
For stationary models (i.e., with $d = 0$) they will converge,
    so that prediction intervals for long horizons are all essentially the same. </br>
For $d \ge 1$,
    the prediction intervals will continue to grow into the future.

As with most prediction interval calculations,
    ARIMA-based intervals tend to be too narrow. </br>
This occurs because only the variation in the errors has been accounted for. </br>
There is also variation in the parameter estimates, and in the model order,
    that has not been included in the calculation. </br>
In addition, the calculation assumes that
    the historical patterns that have been modelled
    will continue into the forecast period.


## Seasonal ARIMA models {#sec-seasonal-arima-models}

So far, we have restricted our attention to
    non-seasonal data and non-seasonal ARIMA models. </br>
However, ARIMA models are also capable of modelling a wide range of seasonal data.

A seasonal ARIMA model is formed by
    including additional seasonal terms
    in the ARIMA models we have seen so far. </br>
It is written as follows:
$$
\begin{array}
    {c c c}
    \text{ARIMA}
    & \underbrace{(p, d, q)}
    & \underbrace{(P, D, Q)_{m}} \\
        & {\uparrow} & {\uparrow} \\
        & \substack{\text{Non-seasonal part} \\ \text{of the model}}
        & \substack{\text{Seasonal part} \\ \text{of the model}}
\end{array}
$$
where
    $m$ = the seasonal period (e.g., number of observations per year). </br>
We use
    uppercase notation for the seasonal parts of the model, and
    lowercase notation for the non-seasonal parts of the model.

The seasonal part of the model consists of terms that are
    similar to the non-seasonal components of the model,
    but involve backshifts of the seasonal period. </br>
**example**, an ARIMA(1,1,1)(1,1,1)$_4$ model (without a constant)
    is for quarterly data ($m = 4$), and can be written as
$$
(1 - \phi_{1} B) ~ (1 - \Phi_{1} B^{4}) (1 - B) (1 - B^{4})y_{t} =
    (1 + \theta_{1} B)~ (1 + \Theta_{1} B^{4})\varepsilon_{t}.
$$

The additional seasonal terms are simply multiplied by the non-seasonal terms.


### ACF/PACF {#sec-acf-pacf}

The seasonal part of an AR or MA model
    will be seen in the seasonal lags of the PACF and ACF.
**example**, an ARIMA(0,0,0)(0,0,1)$_{12}$ model will show:
    a spike at lag 12 in the ACF but no other significant spikes;
    exponential decay in the seasonal lags of the PACF
        (i.e., at lags 12, 24, 36, …). </br>
Similarly, an ARIMA(0,0,0)(1,0,0)$_{12}$ model will show:
    exponential decay in the seasonal lags of the ACF;
    a single significant spike at lag 12 in the PACF.

In considering the appropriate seasonal orders for a seasonal ARIMA model,
    restrict attention to the seasonal lags.

The modelling procedure is almost the same as for non-seasonal data,
    except that we need to select seasonal AR and MA terms
    as well as the non-seasonal components of the model. </br>
The process is best illustrated via examples.


### Example: Monthly US leisure and hospitality employment {#sec-eg-monthly-us-employment}

We will describe seasonal ARIMA modelling using
    monthly US employment data for leisure and hospitality jobs
    from January 2001 to September 2019
```{r}
#| label: fig-monthly-us-employment
#| fig-cap: "Monthly US leisure and hospitality employment, 2001-2019."
leisure <- us_employment %>%
  filter(Title == "Leisure and Hospitality", year(Month) > 2000) %>%
  mutate(Employed = Employed/1000) %>%
  select(Month, Employed)

autoplot(leisure, Employed) +
  labs(title = "US employment: leisure and hospitality", y="Number of people (millions)")
```

The data are clearly non-stationary,
    with strong seasonality and a nonlinear trend,
    so we will first take a seasonal difference. </br>
The seasonally differenced data are shown in
    @fig-monthly-us-employment-seasonally-differenced.
```{r}
#| label: fig-monthly-us-employment-seasonally-differenced
#| fig-cap: "Seasonally differenced Monthly US leisure and hospitality employment."
leisure %>%
  gg_tsdisplay(difference(Employed, 12), plot_type='partial', lag = 36) +
  labs(title="Seasonally differenced", y="")
```

These are also clearly non-stationary,
    so we take a further first difference in
    @fig-monthly-us-employment-double-differenced.
```{r}
#| label: fig-monthly-us-employment-double-differenced
#| fig-cap: "Double differenced Monthly US leisure and hospitality employment."
leisure %>%
  gg_tsdisplay(difference(Employed, 12) %>% difference(), plot_type='partial', lag = 36) +
  labs(title = "Double differenced", y="")
```

Our aim now is to find an appropriate ARIMA model
    based on the ACF and PACF shown in
    @fig-monthly-us-employment-double-differenced. </br>
The significant spike at lag 2 in the ACF suggests
    a non-seasonal MA(2) component. </br>
The significant spike at lag 12 in the ACF suggests
    a seasonal MA(1) component. </br>
Consequently, we begin with an ARIMA(0,1,2)(0,1,1)$_{12}$ model,
    indicating a first difference, a seasonal difference, and
    non-seasonal MA(2) and seasonal MA(1) component. </br>
If we had started with the PACF,
    we may have selected an ARIMA(2,1,0)(0,1,1)$_{12}$ model —
    using the PACF to select the non-seasonal part of the model
    and the ACF to select the seasonal part of the model. </br>
We will also include an automatically selected model. </br>
By setting `stepwise=FALSE` and `approximation=FALSE`,
    we are making R work extra hard to find a good model. </br>
This takes much longer,
    but with only one series to model,
    the extra time taken is not a problem.
```{r}
fit <- leisure %>%
  model(
    arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
    arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
    auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
  )

fit %>% pivot_longer(everything(), names_to = "Model name", values_to = "Orders")

glance(fit) %>% arrange(AICc) %>% select(.model:BIC)
```

The `ARIMA()` function
    uses `unitroot_nsdiffs()` to determine $D$
        (the number of seasonal differences to use),
    and `unitroot_ndiffs()` to determine $d$
        (the number of ordinary differences to use),
    when these are not specified. </br>
The selection of the other model parameters ($p, q, P$ and $Q$)
    are all determined by minimizing the AICc,
    as with non-seasonal ARIMA models.

The three fitted models have similar AICc values,
    with the automatically selected model being a little better. </br>
Our second “guess” of ARIMA(2,1,0)(0,1,1)$_{12}$ turned out to be very close to
    the automatically selected model of ARIMA(2,1,0)(1,1,1)$_{12}$.

The residuals for the best model are shown in
    @fig-monthly-us-employment-arima-residuals.
```{r}
#| label: fig-monthly-us-employment-arima-residuals
#| fig-cap: "Residuals from the fitted ARIMA(2,1,0)(1,1,1)$_{12}$ model."
fit %>% select(auto) %>% gg_tsresiduals(lag = 36)
```

One small but significant spike (at lag 11) out of 36
    is still consistent with white noise. </br>
To be sure, we use a Ljung-Box test,
    which has a large p-value,
    confirming that the residuals are similar to white noise. </br>
Note that the alternative models also pass this test.
```{r}
augment(fit) %>% features(.innov, ljung_box, lag = 24, dof = 4)
```

Thus, we now have a seasonal ARIMA model
    that passes the required checks and is ready for forecasting. </br>
Forecasts from the model for the next three years are shown in
    @fig--monthly-us-employment-arima-forecasts. </br>
The forecasts have captured the seasonal pattern very well,
    and the increasing trend extends the recent pattern. </br>
The trend in the forecasts is induced by the double differencing.
```{r}
#| label: fig--monthly-us-employment-arima-forecasts
#| fig-cap: "Forecasts of monthly US leisure and hospitality employment using the ARIMA(2,1,0)(1,1,1)$_{12}$ model. 80% and 95% prediction intervals are shown. "
forecast(fit, h = 36) %>%
  filter(.model == 'auto') %>%
  autoplot(leisure) +
  labs(title = "US employment: leisure and hospitality", y = "Number of people (millions)")
```


### Example: Corticosteroid drug sales in Australia {#sec-australia-corticosteroid-drug-sales}

For our second example,
    we will try to forecast monthly corticosteroid drug sales in Australia. </br>
These are known as H02 drugs
    under the Anatomical Therapeutic Chemical classification scheme.
```{r}
#| label: fig-australia-corticosteroid-drug-sale
#| fig-cap: "Corticosteroid drug sales in Australia (in millions of scripts per month). Logged data shown in bottom panel."
h02 <- PBS %>%
  filter(ATC2 == "H02") %>%
  summarise(Cost = sum(Cost)/1e6)

h02 %>%
  mutate(log(Cost)) %>%
  pivot_longer(-Month) %>%
  ggplot(aes(x = Month, y = value)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  labs(y = "", title = "Corticosteroid drug scripts (H02)")
```

Data from July 1991 to June 2008 are plotted in
    @fig-australia-corticosteroid-drug-sale. </br>
There is a small increase in the variance with the level,
    so we take logarithms to stabilise the variance.

The data are strongly seasonal and obviously non-stationary,
    so seasonal differencing will be used. </br>
The seasonally differenced data are shown in
    @fig-corticosteroid-seasonally-differenced. </br>
It is not clear at this point whether we should do another difference or not. </br>
We decide not to, but the choice is not obvious.

The last few observations appear to be
    different (more variable) from the earlier data. </br>
This may be due to the fact that
    data are sometimes revised when earlier sales are reported late.
```{r}
#| label: fig-corticosteroid-seasonally-differenced
#| fig-cap: "Seasonally differenced corticosteroid drug sales in Australia (in millions of scripts per month)."
h02 %>% gg_tsdisplay(difference(log(Cost), 12), plot_type='partial', lag_max = 24)
```

In the plots of the seasonally differenced data,
    there are spikes in the PACF at lags 12 and 24,
    but nothing at seasonal lags in the ACF. </br>
This may be suggestive of a seasonal AR(2) term. </br>
In the non-seasonal lags,
    there are three significant spikes in the PACF,
    suggesting a possible AR(3) term. </br>
The pattern in the ACF is not indicative of any simple model.

Consequently, this initial analysis suggests that
    a possible model for these data is an ARIMA(3,0,0)(2,1,0)$_{12}$. </br>
We fit this model, along with some variations on it, and compute the AICc values.
```{r}
fit <- h02 %>%
  model(
    arima301012 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2)),
    arima301111 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(1,1,1)),
    arima301011 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,1)),
    arima301210 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(2,1,0)),
    arima300210 = ARIMA(log(Cost) ~ 0 + pdq(3,0,0) + PDQ(2,1,0)),
    arima302210 = ARIMA(log(Cost) ~ 0 + pdq(3,0,2) + PDQ(2,1,0)),
    arima301110 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(1,1,0))
  )

glance(fit) %>% arrange(AICc) %>% select(.model, AICc)
```

Of these models, the best is the ARIMA(3,0,1)(0,1,2)$_{12}$ model
    (i.e., it has the smallest AICc value). </br>
The innovation residuals from this model are shown in
    @fig-corticosteroid-arima-residuals.
```{r}
fit <- h02 %>%
  model(ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2)))


#| label: fig-corticosteroid-arima-residuals
#| fig-cap: "Innovation residuals from the ARIMA(3,0,1)(0,1,2)$_{12}$ model applied to the H02 monthly script sales data."
fit %>% gg_tsresiduals(lag_max = 36)


augment(fit) %>% features(.innov, ljung_box, lag = 36, dof = 6)
```

There are a few significant spikes in the ACF,
    and the model fails the Ljung-Box test. </br>
The model can still be used for forecasting,
    but the prediction intervals may not be accurate
    due to the correlated residuals.

Next we will try using the automatic ARIMA algorithm. </br>
Running `ARIMA()` with all arguments left at their default values
    led to an ARIMA(2,1,0)(0,1,1)$_{12}$ model. </br>
Running `ARIMA()` with `stepwise=FALSE` and `approximation=FALSE`
    gives an ARIMA(2,1,3)(0,1,1)$_{12}$ model. </br>
However, both models still fail the Ljung-Box test for 36 lags. </br>
**Sometimes it is just not possible to find a model that passes all of the tests.**


### Test set evaluation: {#sec-test-set-evaluation}

We will compare some of the models fitted so far
    using a test set consisting of the last two years of data. </br>
Thus, we fit the models using data from July 1991 to June 2006,
    and forecast the script sales for July 2006 – June 2008.
```{r}
h02_test <- h02 %>% slice(n() - 23:0)
h02_train <- h02 %>% anti_join(h02_test)

fit <- h02_train %>%
  model(
    arima301111 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(1,1,1)),
    arima301012 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2)),
    arima211011 = ARIMA(log(Cost) ~ 0 + pdq(2,1,1) + PDQ(0,1,1)),
    arima212011 = ARIMA(log(Cost) ~ 0 + pdq(2,1,2) + PDQ(0,1,1)),
    arima214011 = ARIMA(log(Cost) ~ 0 + pdq(2,1,4) + PDQ(0,1,1)),
    arima213011 = ARIMA(log(Cost) ~ 0 + pdq(2,1,3) + PDQ(0,1,1)),
    arima301011 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,1)),
    arima302011 = ARIMA(log(Cost) ~ 0 + pdq(3,0,2) + PDQ(0,1,1)),
    arima210011 = ARIMA(log(Cost) ~ 0 + pdq(2,1,0) + PDQ(0,1,1)),
    arima301013 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,3)),
    arima303011 = ARIMA(log(Cost) ~ 0 + pdq(3,0,3) + PDQ(0,1,1)),
    arima302210 = ARIMA(log(Cost) ~ 0 + pdq(3,0,2) + PDQ(2,1,0)),
    arima301210 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(2,1,0)),
    arima210110 = ARIMA(log(Cost) ~ 0 + pdq(2,1,0) + PDQ(1,1,0)),
    arima301110 = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(1,1,0)),
    arima300210 = ARIMA(log(Cost) ~ 0 + pdq(3,0,0) + PDQ(2,1,0))
  )

fc <- fit %>% forecast(new_data = h02_test)

fc %>% accuracy(h02) %>% select(.model, RMSE) %>% arrange(RMSE)
```

The models chosen manually
    are close to the best model over this test set
    based on the RMSE values,
    while those models chosen automatically with `ARIMA()` are not far behind.

When models are compared using AICc values,
    it is important that all models have the same orders of differencing. </br>
However, when comparing models using a test set,
    it does not matter how the forecasts were produced —
    the comparisons are always valid. </br>
Consequently, in the results above,
    we can include some models with only seasonal differencing
    and some models with both first and seasonal differencing,
    while in the earlier results containing AICc values,
    we only compared models with seasonal differencing but no first differencing.

None of the models considered here pass all of the residual tests. </br>
**In practice, we would normally use the best model we could find,**
    **even if it did not pass all of the tests.**

Forecasts from the ARIMA(3,0,1)(0,1,2)$_{12}$ model
    (which has the second lowest RMSE value on the test set,
    and the best AICc value amongst models with only seasonal differencing)
    are shown in @fig-corticosteroid-arima-forecasts.
```{r}
#| label: fig-corticosteroid-arima-forecasts
#| fig-cap: "Forecasts from the ARIMA(3,0,1)(0,1,2)$_{12}$ model applied to the H02 monthly script sales data."
h02 %>%
  model(ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2))) %>%
  forecast() %>%
  autoplot(h02) +
  labs(y = " $AU (millions)", title = "Corticosteroid drug scripts (H02) sales")
```


## ARIMA vs ETS {#sec-arima-vs-ets}

It is a commonly held myth that ARIMA models
    are more general than exponential smoothing. </br>
While linear exponential smoothing models are all special cases of ARIMA models,
    the non-linear exponential smoothing models
    have no equivalent ARIMA counterparts. </br>
On the other hand, there are also many ARIMA models
    that have no exponential smoothing counterparts. </br>
In particular, all ETS models are non-stationary,
    while some ARIMA models are stationary.

![The ETS and ARIMA model classes overlap with the additive ETS models having equivalent ARIMA forms.](./arima_ets_venn.png)
shows the overlap between the two model classes.

The ETS models
    with seasonality or non-damped trend or both
    have two unit roots
    (i.e., they need two levels of differencing to make them stationary). </br>
All other ETS models
    have one unit root
    (they need one level of differencing to make them stationary).

@tbl-ets-arima-equivalence-relationships gives
    the equivalence relationships for the two classes of models. </br>
For the seasonal models,
    the ARIMA parameters have a large number of restrictions.

| ETS model       | ARIMA model                  | Parameters |
| :-------------- | :--------------------------- | :--------- |
| ETS(A,N,N)      | ARIMA(0,1,1)                 | $\theta_1 = \alpha - 1$ |
| ETS(A,A,N)      | ARIMA(0,2,2)                 | $\theta_1 = \alpha + \beta - 2; \\ \theta_2 = 1 - \alpha$ |
| ETS(A,A $_d$,N) | ARIMA(1,1,2)                 | $\phi_1 = \phi; \\ \theta_1 = \alpha + \phi \beta -1 - \phi; \\ \theta_2 =(1 - \alpha)\phi$ |
| ETS(A,N,A)      | ARIMA(0,1,$m$)(0,1,0)$_m$    | |
| ETS(A,A,A)      | ARIMA(0,1,$m+1$)(0,1,0)$_m$  | |
| ETS(A,A $_d$,A) | ARIMA(1,0,$m+1$)(0,1,0)$_m$  | |

: Equivalence relationships between ETS and ARIMA models. {#tbl-ets-arima-equivalence-relationships}

The AICc is useful for selecting between models in the same class. </br>
**example**,
    we can use it to select
    an ARIMA model between candidate ARIMA models
    or an ETS model between candidate ETS models. </br>
As already noted, comparing information criteria
    is only valid for ARIMA models of the same orders of differencing. </br>
However, it cannot be used to compare between ETS and ARIMA models
    because they are in different model classes, 
    and the likelihood is computed in different ways. </br>
The examples below demonstrate selecting between these classes of models.


### Comparing ARIMA() and ETS() on non-seasonal data {#sec-comparing-arima-ets-non-seasonal-data}

We can use time series cross-validation to compare ARIMA and ETS models. </br>
Let’s consider the Australian population from the `global_economy` dataset,
    as introduced in @sec-methods-with-trend (Chapter 8).

```{r}
aus_economy <- global_economy %>%
  filter(Code == "AUS") %>%
  mutate(Population = Population/1e6)

aus_economy %>%
  slice(-n()) %>%
  stretch_tsibble(.init = 10) %>%
  model(
    ETS(Population),
    ARIMA(Population)
  ) %>%
  forecast(h = 1) %>%
  accuracy(aus_economy) %>%
  select(.model, RMSE:MAPE)
```

In this case the ETS model has higher accuracy
    on the cross-validated performance measures. </br>
Below we generate and plot forecasts for the next 5 years
    generated from an ETS model.
```{r}
#| label: fig-australian-population-forecast-ets
#| fig-cap: "Forecasts from an ETS model fitted to the Australian population."
aus_economy %>%
  model(ETS(Population)) %>%
  forecast(h = "5 years") %>%
  autoplot(aus_economy %>% filter(Year >= 2000)) +
  labs(title = "Australian population", y = "People (millions)")
```


### Comparing ARIMA() and ETS() on seasonal data {#sec-comparing-arima-ets-seasonal-data}

In this case we want to compare seasonal ARIMA and ETS models
    applied to the quarterly cement production data (from `aus_production`). </br>
Because the series is relatively long,
    we can afford to use a training and a test set
    rather than time series cross-validation. </br>
The advantage is that this is much faster. </br>
We create a training set from the beginning of 1988 to the end of 2007 and
    select an ARIMA and an ETS model using the `ARIMA()` and `ETS()` functions.
```{r}
cement <- aus_production %>%
  select(Cement) %>%
  filter_index("1988 Q1" ~ .)

train <- cement %>% filter_index(. ~ "2007 Q4")
```

The output below shows the model selected and estimated by `ARIMA()`. </br>
The ARIMA model does well in capturing all the dynamics in the data
    as the residuals seem to be white noise.
```{r}
fit_arima <- train %>% model(ARIMA(Cement))

report(fit_arima)


#| label: fig-australian-production-residuals-arima
#| fig-cap: "Residual diagnostic plots for the ARIMA model fitted to the quarterly cement production training data."
fit_arima %>% gg_tsresiduals(lag_max = 16)


augment(fit_arima) %>%
  features(.innov, ljung_box, lag = 16, dof = 6)
```

The output below also shows the ETS model selected and estimated by `ETS()`. </br>
This model also does well in capturing all the dynamics in the data,
    as the residuals similarly appear to be white noise.
```{r}
fit_ets <- train %>% model(ETS(Cement))

report(fit_ets)


#| label: fig-australian-production-residuals-ets
#| fig-cap: "Residual diagnostic plots for the ETS model fitted to the quarterly cement production training data."
fit_ets %>%
  gg_tsresiduals(lag_max = 16)


augment(fit_ets) %>%
  features(.innov, ljung_box, lag = 16, dof = 6)
```

The output below evaluates the forecasting performance
    of the two competing models over the test set. </br>
In this case the ARIMA model seems to be the slightly more accurate model
    based on the test set RMSE, MAPE and MASE.

Generate forecasts and compare accuracy over the test set
```{r}
bind_rows(
  fit_arima %>% accuracy(),
  fit_ets %>% accuracy(),
  fit_arima %>% forecast(h = 10) %>% accuracy(cement),
  fit_ets %>% forecast(h = 10) %>% accuracy(cement)
) %>%
  select(-ME, -MPE, -ACF1)
```

Below we generate and plot forecasts from the ARIMA model for the next 3 years.
```{r}
#| label: fig-australian-production-forecasts-arima
#| fig-cap: "Forecasts from an ARIMA model fitted to all of the available quarterly cement production data since 1988."
cement %>%
  model(ARIMA(Cement)) %>%
  forecast(h="3 years") %>%
  autoplot(cement) +
  labs(title = "Cement production in Australia", y = "Tonnes ('000)")
```


## Exercises


## Future Reading
