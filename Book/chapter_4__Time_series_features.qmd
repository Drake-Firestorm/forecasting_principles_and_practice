---
title: "Time Series Features"
format: 
  html:
    code-fold: true
number-sections: true
---

`feasts` package includes functions for computing
    **FEatures And Statistics from Time Series** (hence the name). </br>
**time series features** are *numerical summaries computed from the series*. </br>
**e.g.** autocorrelations, Guerrero estimate. </br>
We can compute many different features on many different time series,
    and use them to explore the properties of the series.


## Some simple statistics {#sec-some-simple-statistics}

Any numerical summary computed from a time series is a feature of that time series. </br>
**example** - the mean, minimum or maximum

> `features()` </br>
    compute the features

```{r}
tourism %>% features(Trips, mean)
```
 
It is useful to give the resulting feature columns names
    to help us remember where they came from. </br>
This can be done by using a list of functions.
```{r}
tourism %>% features(Trips, list(mean=mean)) %>% arrange(mean)
```

Rather than compute one feature at a time,
    it is convenient to compute many features at once. </br>
A common short summary of a data set is to compute five summary statistics:
- the minimum, 
- first quartile, 
- median, 
- third quartile and 
- maximum. </br>
These divide the data into four equal-size sections,
    each containing 25% of the data. </br>
The `quantile()` function can be used to compute them.

```{r}
tourism %>% features(Trips, quantile, prob = seq(0, 1, by = 0.25))
# below gives the same result
tourism %>% features(Trips, quantile)   
```


## ACF Features {#sec-acf-features}

Autocorrelations were discussed in @sec-autocorrelation (Chapter 2).
All the autocorrelations of a series can be considered features of that series. </br>
We can also summarise the autocorrelations to produce new features. </br>
**example**,
    the sum of the **first ten squared autocorrelation coefficients**
    is a useful summary of how much autocorrelation there is in a series,
    regardless of lag.

We can also compute autocorrelations of transformations of a time series. </br>
A **useful transformation** in this context
    is to look at changes in the series between periods. </br>
That is, we *difference* the data and create a new time series
    consisting of the differences between consecutive observations. </br>
Then we can compute the autocorrelations of this new differenced series. </br>
Occasionally it is **useful** to apply the same differencing operation again,
    so we compute the differences of the differences. </br>
The autocorrelations of this double differenced series may provide useful information.

Another **related approach** is to compute seasonal differences of a series. </br>
**example**
    If we had monthly data, we would compute the difference between
    consecutive Januaries, consecutive Februaries, and so on. </br>
This enables us to look at how the series is changing between years,
    rather than between months. </br>
Again, the autocorrelations of the seasonally differenced series
    may provide useful information.

`feat_acf()` - *computes a selection of the autocorrelations* discussed here. </br>
It will return six or seven features:
- the first autocorrelation coefficient from the original data;
- the sum of square of the first ten autocorrelation coefficients from the original data;
- the first autocorrelation coefficient from the differenced data;
- the sum of square of the first ten autocorrelation coefficients from the differenced data;
- the first autocorrelation coefficient from the twice differenced data;
- the sum of square of the first ten autocorrelation coefficients from the twice differenced data;
- For seasonal data, the autocorrelation coefficient at the first seasonal lag is also returned.
 
```{r}
tourism %>% features(Trips, feat_acf)
```


## STL Features {#sec-stl-features}

STL decomposition discussed in
    [Chapter 3](./chapter_3_time_series_decomposition.qmd)
    is the basis for several more features.

time series decomposition can be used to measure
    the strength of trend and seasonality in a time series. </br>
For strongly trended data
    the seasonally adjusted data should have much more **variation**
    than the remainder component. </br>
Therefore $\text{Var}(R_t) / \text{Var}(T_t + R_t)$ should be relatively small. </br>
But for data with little or no trend,
    the two variances should be approximately the same. </br>
So we define the **strength of trend** as:
$F_T = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(T_t + R_t)}\right)$. </br>
This will give a measure of the strength of the trend between 0 and 1. </br>
Because the variance of the remainder might occasionally be even larger
    than the variance of the seasonally adjusted data,
    we set the minimal possible value of $F_T$ equal to zero.

The **strength of seasonality** is defined similarly,
    but with respect to the detrended data
    rather than the seasonally adjusted data:
$F_S = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(S_t + R_t)}\right)$. </br>
A series with seasonal strength $F_S$ close to 0 
    exhibits almost no seasonality, while
    a series with strong seasonality
        will have $F_S$ close to 1
        because $\text{Var}(Rt)$ will be much smaller than $\text{Var}(St + Rt)$.

These measures can be useful, for **example**,
    when you have a large collection of time series, 
    and you need to find the series with the most trend or the most seasonality.

*Other useful features based on STL* include
    the timing of peaks and troughs -
        which month or quarter contains the largest seasonal component
        and which contains the smallest seasonal component. </br>
This tells us something about the nature of the seasonality.

`feat_stl()` - *compute these STL-based features*

We can then use these features in plots
    to identify what type of series are heavily trended and
    what are most seasonal.
```{r}
#| label: fig-tourism-seasonal-vs-trend-strength
#| fig-cap: "Seasonal strength vs trend strength for all tourism series."
tourism %>%
  features(Trips, feat_stl) %>%
  ggplot(aes(x = trend_strength, y = seasonal_strength_year, col = Purpose)) +
  geom_point() + facet_wrap(vars(State))
```

Clearly, holiday series are most seasonal which is unsurprising. </br>
The strongest trends tend to be in Western Australia.
 
The most seasonal series can also be easily identified and plotted.
```{r}
#| label: fig-australian-tourism-most-seasonal-series
#| fig-cap: "The most seasonal series in the Australian tourism data."
tourism %>%
  features(Trips, feat_stl) %>%
  filter(seasonal_strength_year == max(seasonal_strength_year)) %>%
  left_join(tourism, by = c("State","Region","Purpose")) %>%
  ggplot(aes(x = Quarter, y = Trips)) + geom_line() +
  facet_grid(vars(State,Region,Purpose))
```

This shows holiday trips to the most popular ski region of Australia.

`feat_stl()` function
    returns several more features other than those discussed above. </br>
- `spikiness`
    measures the prevalence of spikes
        in the remainder component $R_t$ of the STL decomposition.
    It is the variance of the leave-one-out variances of $R_t$.
- `linearity`
    measures the linearity of the trend component of the STL decomposition.
    It is based on the coefficient of a linear regression
        applied to the trend component.
- `curvature`
    measures the curvature of the trend component of the STL decomposition.
    It is based on the coefficient from an orthogonal quadratic regression
        applied to the trend component.
- `stl_e_acf1`
    is the first autocorrelation coefficient of the remainder series.
- `stl_e_acf10`
    is the sum of squares of the first ten autocorrelation coefficients
        of the remainder series.


## Other Features {#sec-other-features}

Many more features are possible,
    and the `feasts` package computes only a few dozen features
    that have proven useful in time series analysis. </br>
It is also easy to add your own features by writing an R function
    that takes a univariate time series input
    and returns a numerical vector containing the feature values.

remaining features in the `feasts` package. </br>
details of some of them are discussed later in the book.
- `coef_hurst`
    will calculate the **Hurst coefficient** of a time series 
    which is a *measure of long memory.*
    A series with **long memory** 
        will have significant autocorrelations for many lags.
- `feat_spectral`
    will compute the **(Shannon) spectral entropy of a time series**,
        which is a *measure of how easy the series is to forecast*.
    A series which has strong trend and seasonality
        (and so is easy to forecast)
        will have *entropy close to 0*. 
    A series that is very noisy
        (and so is difficult to forecast)
        will have *entropy close to 1*.
- `box_pierce`
    gives the **Box-Pierce statistic**
        *for testing if a time series is white noise*,
    and the corresponding p-value.
    discussed in @sec-residual-diagnostics (Chapter 5).
- `ljung_box`
    gives the **Ljung-Box statistic**
        *for testing if a time series is white noise*,
    and the corresponding p-value.
    discussed in @sec-residual-diagnostics (Chapter 5).
- The `kth partial autocorrelations`
    *measure the relationship between observations $k$ periods apart*
        *after removing the effects of observations between them*.
    So the first partial autocorrelation ($k=1$)
        is identical to the first autocorrelation,
        because there is nothing between them to remove.
    The `feat_pacf` function contains several features
        involving partial autocorrelations including
        - the sum of squares of the first five partial autocorrelations
            for the original series,
        - the first-differenced series and
        - the second-differenced series.
    For seasonal data, it also includes
        - the partial autocorrelation at the first seasonal lag. 
    discussed in @sec-non-seasonal-arima-models (Chapter 9).
- `unitroot_kpss`
    gives the **Kwiatkowski-Phillips-Schmidt-Shin (KPSS) statistic**
        *for testing if a series is stationary*,
    and the corresponding p-value. 
    discussed in @sec-stationarity-differencing (Chapter 9).
- `unitroot_pp`
    gives the **Phillips-Perron statistic**
        *for testing if a series is non-stationary*,
    and the corresponding p-value.
- `unitroot_ndiffs`
    *gives the number of differences required*
        *to lead to a stationary series*
        based on the **KPSS test**.
    discussed in @sec-stationarity-differencing (Chapter 9).
- `unitroot_nsdiffs`
    *gives the number of seasonal differences required to make a series stationary*.
    discussed in @sec-stationarity-differencing (Chapter 9).
- `var_tiled_mean`
    *gives the variances of the **tiled means***
        (i.e., the means of consecutive non-overlapping blocks of observations).
    The default tile length is either
        10 (for non-seasonal data) 
        or the length of the seasonal period.
    This is sometimes called the **stability** feature.
- `var_tiled_var`
    *gives the variances of the **tiled variances***
        (i.e., the variances of consecutive non-overlapping blocks of observations).
    This is sometimes called the **lumpiness** feature.
- `shift_level_max`
    *finds the largest mean shift*
        *between two consecutive sliding windows of the time series*.
    This is useful for finding sudden jumps or drops in a time series.
- `shift_level_index`
    *gives the index at which the largest mean shift occurs*.
- `shift_var_max`
    *finds the largest variance shift*
        *between two consecutive sliding windows of the time series*.
    This is useful for finding sudden changes in the volatility of a time series.
- `shift_var_index`
    *gives the index at which the largest mean shift occurs*
- `shift_kl_max`
    *finds the largest distributional shift*
        (based on the **Kulback-Leibler divergence**)
        *between two consecutive sliding windows of the time series*.
    This is useful for finding sudden changes in the distribution of a time series.
- `shift_kl_index`
    *gives the index at which the largest KL shift occurs*.
- `n_crossing_points`
    *computes the number of times a time series crosses the median*.
- `longest_flat_spot`
    *computes the number of sections of the data*
        *where the series is relatively unchanging*.
- `stat_arch_lm`
    returns the statistic based on the
    **Lagrange Multiplier (LM) test** of Engle (1982)
    for **autoregressive conditional heteroscedasticity (ARCH)**.
- `guerrero`
    computes the optimal $\lambda$ value for a
        **Box-Cox transformation using the Guerrero method**
    (discussed in @sec-transformations-adjustments (Chapter 3)).


## Exploring Australian tourism data {#sec-exploring-australian-tourism-data}

All of the features included in the `feasts` package
    can be computed in one line like this.
```{r}
tourism_features <- tourism %>%
  features(Trips, feature_set(pkgs="feasts"))

tourism_features
```

Provided the `urca` and `fracdiff` packages are installed,
    this gives 48 features for every combination of the three key variables
    (`Region`, `State` and `Purpose`). </br>
We can treat this tibble like any data set
    and analyse it find interesting observations or groups of observations.

```{r}
#| label: fig-australian-tourism-pairwise-plots-seasonal-features
#| fig-cap: "Pairwise plots of all the seasonal features for the Australian tourism data"
tourism_features %>%
  select_at(vars(contains("season"), Purpose)) %>%
  mutate(
    seasonal_peak_year = glue::glue("Q{seasonal_peak_year+1}"),
    seasonal_trough_year = glue::glue("Q{seasonal_trough_year+1}"),
  ) %>%
  GGally::ggpairs(mapping = aes(colour = Purpose))
```

Here, the `Purpose` variable is mapped to colour. </br>
There is a lot of information in this figure,
    and we will highlight just a few things we can learn.
- The three numerical measures related to seasonality
    (`seasonal_strength_year`, `season_acf1` and `season_pacf`)
    are all positively correlated.
- The bottom left panel and the top right panel both show that
    the most strongly seasonal series are related to holidays (as we saw previously).
- The bar plots in the bottom row of the
    `seasonal_peak_year` and `seasonal_trough_year` columns show that
    seasonal peaks in Business travel occur
    most often in Quarter 3, and least often in Quarter 1.

It is difficult to explore more than a handful of variables in this way. </br>
A useful way to handle many more variables is to use
    a dimension reduction technique such as **principal components**. </br>
This gives linear combinations of variables
    that explain the most variation in the original data.

We can compute the principal components of the tourism features as follows.
```{r}
pcs <- tourism_features %>%
  select(-State, -Region, -Purpose) %>%
  prcomp(scale = TRUE) %>%
  augment(tourism_features)


#| label: fig-australian-tourism-principal-components
#| fig-cap: "A plot of the first two principal components, calculated from the 48 features of the Australian quarterly tourism data."
pcs %>%
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = Purpose)) +
  geom_point() + theme(aspect.ratio = 1)
```

Each point on @fig-australian-tourism-principal-components represents one series
    and its location on the plot is based on all 48 features. </br>
The first principal component (`.fittedPC1`)
    is the linear combination of the features
    which explains the most variation in the data. </br>
The second principal component (`.fittedPC2`)
    is the linear combination which explains the next most variation in the data,
    while being uncorrelated with the first principal component. 

@fig-australian-tourism-principal-components
    reveals a few things about the tourism data. </br>
First, the holiday series behave quite differently from the rest of the series. </br>
Almost all of the holiday series appear in the top half of the plot,
    while almost all of the remaining series appear in the bottom half of the plot. </br>
Clearly, the second principal component is distinguishing between
    holidays and other types of travel.

The plot also allows us to identify anomalous time series -
    series which have unusual feature combinations. </br>
These appear as points that are separate from the majority of series
    in @fig-australian-tourism-principal-components. </br>
There are four which stand out,
    and we can identify which series they correspond to as follows.
```{r}
outliers <- pcs %>%
  filter(.fittedPC1 > 10.5) %>%
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)


#| label: fig-australian-tourism-anomalous-time-series
#| fig-cap: "Four anomalous time series from the Australian tourism data."
outliers %>%
  left_join(tourism, by = c("State", "Region", "Purpose")) %>%
  mutate(Series = glue::glue("{State}", "{Region}", "{Purpose}", .sep = "\n\n")) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(Series ~ ., scales = 'free') +
  ggtitle("Outlying time series in PC space")
```

We can speculate why these series are identified as unusual.
- Holiday visits to the south coast of NSW
    is highly seasonal but has almost no trend,
    whereas most holiday destinations in Australia show some trend over time.
- Melbourne is an unusual holiday destination because
    it has almost no seasonality,
    whereas most holiday destinations in Australia have highly seasonal tourism.
- The north western corner of Western Australia is unusual because
    it shows an increase in business tourism in the last few years of data,
    but little or no seasonality.
- The south western corner of Western Australia is unusual because
    it shows both an increase in holiday tourism in the last few years of data
    and a high level of seasonality.


## Exercises


## Future Reading
