---
title: "Exponential smoothing"
format:
  html:
    code-fold: true
number-sections: true
---

proposed in the late 1950s (Brown, 1959; Holt, 1957; Winters, 1960),
    and has motivated some of the most successful forecasting methods. </br>
Forecasts produced using exponential smoothing methods
    are weighted averages of past observations,
    with the weights decaying exponentially as the observations get older. </br>
In other words,
    the more recent the observation
    the higher the associated weight. </br>
This framework generates reliable forecasts quickly
    and for a wide range of time series,
    which is a great advantage and of major importance to applications in industry.

The selection of the method is generally based on
    recognising key components of the time series (trend and seasonal)
    and the way in which these enter the smoothing method
    (e.g., in an additive, damped or multiplicative manner).


[An Introduction to Time Series Smoothing in R](https://boostedml.com/2020/05/an-introduction-to-time-series-smoothing-in-r.html)

What is **smoothing**? </br>
A time series can generally be decomposed into several components:
- trend,
- cyclical,
- seasonal, and
- noise.

These are in order of increasing frequency. </br>
As a blog owner, one way I see these is as follows. </br>
**Noise**
    is simply the random variation in daily views. </br>
The **seasonal component**
    is that there are fewer viewers on the weekends. </br>
The **cyclical**
    is that Google’s algorithms cause readership to go up and down, and </br>
the **trend**
    is that over time, readership tends to go up.


[Triple Exponential Smoothing Forecasting](https://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/)

Why is it called **smoothing**? </br>
To the best of my understanding this simply refers to
    the effect these methods have on a graph if you were to plot the values:
    jagged lines become smoother. </br>
Moving average also has the same effect,
    so it deserves the right to be called smoothing just as well.

Alternate names:
- **Level** / **Expected value** / **baseline** / **intercept** (as in Y-intercept)
- **Trend** / **Slope**


## Simple exponential smoothing {#sec-simple-exponential-smoothing}

simplest of the exponentially smoothing methods is naturally called
    **simple exponential smoothing (SES)**. </br>
This method is suitable for forecasting data
    with no clear trend or seasonal pattern.

For **example**,
    the data in @fig-algeria-exports do not display
    any clear trending behaviour or any seasonality. </br>
We have already considered the **naïve** and the **average**
    as possible methods for forecasting such data. </br>
```{r}
algeria_economy <- global_economy %>%
  filter(Country == "Algeria")


#| label: fig-algeria-exports
#| fig-cap: "Exports of goods and services from Algeria from 1960 to 2017."
algeria_economy %>%
  autoplot(Exports) +
  labs(y = "% of GDP", title = "ExportsAlgeria")
```

Using the **naïve** method,
    all forecasts for the future are equal to the last observed value of the series,
$\hat{y}_{T+h|T} = y_{T},$ for $h = 1, 2, \dots$. </br>
Hence, the **naïve** method assumes that
    the most recent observation is the only important one,
    and all previous observations provide no information for the future. </br>
This can be thought of as a weighted average
    where all of the weight is given to the last observation.

Using the **average** method,
    all future forecasts are equal to a simple average of the observed data,
$ \hat{y}_{T+h|T} = \frac{1}{T} \sum_{t=1}^T y_t,$ for $h = 1, 2, \dots$.

Hence, the average method assumes
    that all observations are of equal importance,
    and gives them equal weights when generating forecasts.

We often want something between these two extremes. </br>
**example**,
    it may be sensible to attach larger weights to more recent observation
    than to observations from the distant past. </br>
This is exactly the concept behind **simple exponential smoothing**. </br>
Forecasts are calculated using weighted averages,
    where the weights decrease exponentially
    as observations come from further in the past —
    the smallest weights are associated with the oldest observations.
$$
\hat{y}_{T+1|T} =
    \alpha y_T + \alpha(1-\alpha) y_{T-1}
    + \alpha(1-\alpha)^2 y_{T-2}+ \cdots
$$ {#eq-simple-exponential-smoothing}
where
    $0 \le \alpha \le 1$ is the smoothing parameter. </br>
The one-step-ahead forecast for time $T+1$
    is a weighted average of all of the observations in the series $y_1, \dots, y_T$. </br>
The rate at which the weights decrease is controlled by the parameter $\alpha$.

For any $\alpha$ between 0 and 1,
    the weights attached to the observations decrease exponentially
    as we go back in time,
    hence the name **exponential smoothing**. </br>
If **$\alpha$ is small (i.e., close to 0)**,
    more weight is given to observations from the more distant past. </br>
If **$\alpha$ is large (i.e., close to 1)**,
    more weight is given to the more recent observations. </br>
For the extreme case where **$\alpha=1$**,
    $\hat{y}_{T+1|T} = y_T$,
    and the forecasts are equal to the naïve forecasts.

We present two equivalent forms of simple exponential smoothing,
    each of which leads to the forecast @eq-simple-exponential-smoothing.


### Weighted average form {#sec-weighted-average-form}

The forecast at time $T+1$ is equal to
    a weighted average between the most recent observation $y_T$
    and the previous forecast $\hat{y}_{T|T-1}$
$\hat{y}_{T+1|T} = \alpha y_T + (1-\alpha) \hat{y}_{T|T-1},$
where
    $0 \le \alpha \le 1$ is the smoothing parameter. </br>
Similarly, we can write the fitted values as
$\hat{y}_{t+1|t} = \alpha y_t + (1-\alpha) \hat{y}_{t|t-1},$ for $t=1,\dots,T$.
(Recall that fitted values are simply one-step forecasts of the training data.)

The process has to start somewhere,
    so we let the first fitted value at time 1 be denoted by $\ell_0$
    (which we will have to estimate). </br>
Then
$$
\begin{align*}
    \hat{y}_{2|1} &= \alpha y_1 + (1-\alpha) \ell_0 \\
    \hat{y}_{3|2} &= \alpha y_2 + (1-\alpha) \hat{y}_{2|1} \\
    \hat{y}_{4|3} &= \alpha y_3 + (1-\alpha) \hat{y}_{3|2} \\
    \vdots \\
    \hat{y}_{T|T-1} &= \alpha y_{T-1} + (1-\alpha) \hat{y}_{T-1|T-2} \\
    \hat{y}_{T+1|T} &= \alpha y_T + (1-\alpha) \hat{y}_{T|T-1}.
\end{align*}
$$

Substituting each equation into the following equation, we obtain
$$
\begin{align*}
    \hat{y}_{3|2}
        & = \alpha y_2 + (1-\alpha) \left[\alpha y_1 + (1-\alpha) \ell_0\right] \\
        & = \alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0 \\
    \hat{y}_{4|3}
        & = \alpha y_3 + (1-\alpha) [\alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0] \\
        & = \alpha y_3 + \alpha(1-\alpha) y_2 + \alpha(1-\alpha)^2 y_1 + (1-\alpha)^3 \ell_0 \\
    & \vdots \\
    \hat{y}_{T+1|T}
        & = \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^T \ell_{0}.
\end{align*}
$$

The last term becomes tiny for large $T$. </br>
So, the weighted average form leads to
    the same forecast @eq-simple-exponential-smoothing.


### Component form {#sec-component-form}

An alternative representation is the component form. </br>
For **simple exponential smoothing**,
    the only component included is the level, $\ell_t$. </br>
(Other methods which are considered later in this chapter may also include
    a trend $b_t$ and
    a seasonal component $s_t$.) </br>
**Component form** representations of exponential smoothing methods
    comprise a forecast equation
    and a smoothing equation
    for each of the components included in the method. </br>
The component form of simple exponential smoothing is given by
$$
\begin{align*}
    \text{Forecast equation} &&
        \hat{y}_{t+h|t} & = \ell_{t} \\
    \text{Smoothing equation} &&
        \ell_{t} & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
$$
where
    $\ell_t$ is the level (or the smoothed value) of the series at time $t$. </br>
Setting $h=1$ gives the fitted values, while
    setting $t=T$ gives the true forecasts beyond the training data.

The forecast equation shows that
    the forecast value at time $t+1$
    is the estimated level at time $t$. </br>
The smoothing equation for the level
    (usually referred to as the **level equation**)
    gives the estimated level of the series at each period $t$.

If we replace
    $\ell_t$ with $\hat{y}_{t+1|t}$ and
    $\ell_{t−1}$ with $\hat{y}_{T|T-1}$
    in the smoothing equation,
    we will recover the weighted average form of simple exponential smoothing.

The component form of simple exponential smoothing
    is not particularly useful on its own,
    but it will be the easiest form to use
    when we start adding other components.


### Flat forecasts {#sec-flat-forecasts}

Simple exponential smoothing has a “flat” forecast function:
$\hat{y}_{T+h|T} = \hat{y}_{T+1|T} = \ell_T, \qquad h=2,3,\dots.$ </br>
That is,
    all forecasts take the same value,
    equal to the last level component. </br>
Remember that these forecasts will only be suitable
    if the time series has no trend or seasonal component.


### Optimisation {#sec-optimisation}

The application of every exponential smoothing method requires
    the smoothing parameters and
    the initial values to be chosen. </br>
In particular, for simple exponential smoothing,
    we need to select the values of
    $\alpha$ and $\ell_0$. </br>
All forecasts can be computed from the data once we know those values. </br>
For the methods that follow there is usually
    more than one smoothing parameter and
    more than one initial component to be chosen.

In some cases, the smoothing parameters may be chosen in a subjective manner
    — the forecaster specifies the value of the smoothing parameters
    based on previous experience. </br>
However, a more reliable and objective way
    to obtain values for the unknown parameters
    is to estimate them from the observed data.

In @sec-least-squares-estimation,
    we estimated the coefficients of a regression model
    by minimising the sum of the squared residuals
    (usually known as **SSE** or **sum of squared errors**). </br>
Similarly, the unknown parameters and the initial values
    for any exponential smoothing method
    can be estimated
    by minimising the **SSE**. </br>
The residuals are specified as
$e_t = y_t - \hat{y}_{t|t-1}$ for $t = 1, \dots, T$. </br>
Hence, we find the values of
    the unknown parameters and the initial values that minimise
$$
\text{SSE}
    = \sum_{t=1}^T (y_t - \hat{y}_{t|t-1})^2
    = \sum_{t=1}^T e_t^2.
$$ {#eq-sse}

Unlike the regression case
    (where we have formulas which return
    the values of the regression coefficients that minimise the SSE),
    this involves a non-linear minimisation problem,
    and we need to use an optimisation tool to solve it.


### Example: Algerian exports {#sec-eg-algerian-exports}

```{r}
# Estimate parameters
fit <- algeria_economy %>%
  model(ETS(Exports ~ error("A") + trend("N") + season("N")))

fc <- fit %>%
  forecast(h = 5)

tidy(fit)
```

This gives parameter estimates $\hat{\alpha}=0.84$ and $\hat{ℓ}_0=39.5$,
    obtained by minimising SSE over periods $t = 1, 2, \dots, 58$,
    subject to the restriction that $0 \le \alpha \le 1$.

The black line in @fig-algeria-exports-simple-exponential-smoothing shows the data,
    which has a changing level over time.
```{r}
#| label: fig-algeria-exports-simple-exponential-smoothing
#| fig-cap: "Simple exponential smoothing applied to exports from Algeria (1960–2017). The orange curve shows the one-step-ahead fitted values."
fc %>%
  autoplot(algeria_economy) +
  geom_line(aes(y = .fitted), col="#D55E00", data = augment(fit)) +
  labs(y="% of GDP", title="ExportsAlgeria") +
  guides(colour = FALSE)
```

The forecasts for the period 2018–2022
    are plotted in @fig-algeria-exports-simple-exponential-smoothing. </br>
Also plotted are one-step-ahead fitted values
    alongside the data over the period 1960–2017. </br>
The large value of $\alpha$ in this example
    is reflected in the large adjustment that takes place
    in the estimated level $\ell_t$ at each time. </br>
A smaller value of $\alpha$ would lead to smaller changes over time,
    and so the series of fitted values would be smoother.

The prediction intervals shown here
    are calculated using the methods described in @sec-forecasting-ets-models. </br>
The prediction intervals show that
    there is considerable uncertainty in the future exports
    over the five-year forecast period. </br>
So interpreting the point forecasts
    without accounting for the large uncertainty
    can be very misleading.


## Methods with trend {#sec-methods-with-trend}

### Holt’s linear trend method {#sec-holts-linear-trend-method}

Holt (1957) extended simple exponential smoothing
    to allow the forecasting of data with a trend. </br>
This method involves
    a forecast equation and
    two smoothing equations
        (one for the level and
        one for the trend)
$$
\begin{align*}
    \text{Forecast equation} &&
        \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
    \text{Level equation} &&
        \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\
    \text{Trend equation} &&
        b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1},
\end{align*}
$$
where
    $\ell_t$ denotes an estimate of the level of the series at time $t$,
    $b_t$ denotes an estimate of the trend (slope) of the series at time $t$,
    $\alpha$ is the smoothing parameter for the level, $0 \le \alpha \le 1$, and
    $\beta^*$ is the smoothing parameter for the trend, $0 \le \beta^* \le 1$.
        (We denote this as $\beta^*$ instead of $\beta$
        for reasons that will be explained in
        @sec-innovations-state-space-models-exponential-smoothing.)

As with simple exponential smoothing,
    the level equation here shows that
    $\ell_t$ is a weighted average of
    observation $y_t$ and the one-step-ahead training forecast for time $t$,
    here given by $\ell_{t-1} + b_{t-1}$. </br>
The trend equation shows that
    $b_t$ is a weighted average of the estimated trend at time $t$
    based on $\ell_{t} - \ell_{t-1}$ and $b_{t-1}$,
    the previous estimate of the trend.

The forecast function is no longer flat but trending. </br>
The $h$-step-ahead forecast is equal to
    the last estimated level
    plus $h$ times the last estimated trend value. </br>
Hence the forecasts are a linear function of $h$.


### Example: Australian population {#sec-eg-australian-population}

```{r}
aus_economy <- global_economy %>%
  filter(Code == "AUS") %>%
  mutate(Pop = Population / 1e6)

#| label: fig-australian-population
#| fig-cap: "Australia’s population, 1960-2017."
autoplot(aus_economy, Pop) +
  labs(y = "Millions", title = "Australian population")
```

We will apply Holt’s method to this series. </br>
The smoothing parameters, $\alpha$ and $\beta^*$,
    and the initial values $\ell_0$ and $b_0$
    are estimated by minimising the SSE
    for the one-step training errors as in @sec-simple-exponential-smoothing.

```{r}
fit <- aus_economy %>%
  model(AAN = ETS(Pop ~ error("A") + trend("A") + season("N")))

fc <- fit %>% forecast(h = 10)

tidy(fit) %>% print.data.frame()
```

The estimated smoothing coefficient for the level is $\hat{\alpha} = 0.9999$. </br>
The very high value shows that
    the level changes rapidly in order to capture the highly trended series. </br>
The estimated smoothing coefficient for the slope is $\hat{\beta}^* = 0.3267$. </br>
This is relatively large suggesting that
    the trend also changes often (even if the changes are slight).


### Damped trend methods {#sec-damped-trend-methods}

The forecasts generated by Holt’s linear method
    display a constant trend (increasing or decreasing)
    indefinitely into the future. </br>
Empirical evidence indicates that these methods tend to over-forecast,
    especially for longer forecast horizons. </br>
Motivated by this observation, Gardner & McKenzie (1985)
    introduced a parameter that “dampens” the trend to a flat line
    some time in the future. </br>
Methods that include a damped trend have proven to be very successful,
    and are arguably the most popular individual methods
    when forecasts are required automatically for many series.

In conjunction with the smoothing parameters $\alpha$ and $\beta^*$
    (with values between 0 and 1 as in Holt’s method),
    this method also includes a damping parameter $0<\phi<1$:
$$
\begin{align*}
    \hat{y}_{t+h|t} &= \ell_{t} + (\phi + \phi^2 + \dots + \phi^{h})b_{t} \\
    \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1}) \\
    b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}.
\end{align*}
$$

If $\phi = 1$, the method is identical to Holt’s linear method. </br>
For values between $0$ and $1$, $\phi$ dampens the trend so that
    it approaches a constant some time in the future. </br>
In fact, the forecasts converge to
    $\ell_T + \phi b_T/(1 - \phi)$
    as $h \rightarrow \infty$ for any value $0 < \phi < 1$. </br>
This means that
    short-run forecasts are trended
    while long-run forecasts are constant.

In practice, $\phi$ is rarely less than 0.8
    as the damping has a very strong effect for smaller values. </br>
Values of $\phi$ close to 1 will mean that
    a damped model is not able to be distinguished from a non-damped model. </br>
For these reasons,
    we usually restrict $\phi$ to a minimum of 0.8 and a maximum of 0.98.


### Example: Australian Population (continued) {#sec-eg-australian-population-continued}

@fig-australian-population-forecast-holts-method shows
    the forecasts for years 2018–2032
    generated from Holt’s linear trend method and the damped trend method.

```{r}
#| label: fig-australian-population-forecast-holts-method
#| fig-cap: "Forecasting annual Australian population (millions) over 2018-2032. For the damped trend method, $\phi=0.90$."
aus_economy %>%
  model(
    `Holt's method` = ETS(Pop ~ error("A") + trend("A") + season("N")),
    `Damped Holt's method` = ETS(Pop ~ error("A") + trend("Ad", phi = 0.9) + season("N"))
  ) %>%
  forecast(h = 15) %>%
  autoplot(aus_economy, level = NULL) +
  labs(title = "Australian population", y = "Millions") +
  guides(colour = guide_legend(title = "Forecast"))
```

We have set the damping parameter to a relatively low number ($\phi$ = 0.90)
    to exaggerate the effect of damping for comparison. </br>
Usually, we would estimate $\phi$ along with the other parameters. </br>
We have also used a rather large forecast horizon ($h = 15$)
    to highlight the difference between a damped trend and a linear trend.


### Example: Internet usage {#sec-eg-internet-usage}

In this example, we compare the forecasting performance
    of the three exponential smoothing methods that we have considered so far
    in forecasting the number of users connected to the internet via a server. </br>
The data is observed over 100 minutes and is shown in @fig-internet-users.

```{r}
www_usage <- as_tsibble(WWWusage)

#| label: fig-internet-users
#| fig-cap: "Users connected to the internet through a server"
www_usage %>% autoplot(value) +
  labs(x="Minute", y="Number of users", title = "Internet usage per minute")
```

We will use time series cross-validation
    to compare the one-step forecast accuracy of the three methods.

```{r}
www_usage %>%
  stretch_tsibble(.init = 10) %>%
  model(
    SES = ETS(value ~ error("A") + trend("N") + season("N")),
    Holt = ETS(value ~ error("A") + trend("A") + season("N")),
    Damped = ETS(value ~ error("A") + trend("Ad") +
                   season("N"))
  ) %>%
  forecast(h = 1) %>%
  accuracy(www_usage)
```

Damped Holt’s method is best whether you compare MAE or RMSE values. </br>
So we will proceed with using the damped Holt’s method
    and apply it to the whole data set to get forecasts for future minutes.

```{r}
fit <- www_usage %>%
  model(Damped = ETS(value ~ error("A") + trend("Ad") + season("N")))

# Estimated parameters:
tidy(fit) %>% print.data.frame()
```

The smoothing parameter for the slope is estimated to be almost one,
    indicating that the trend changes to mostly reflect
    the slope between the last two minutes of internet usage. </br>
The value of $\alpha$ is very close to one,
    showing that the level reacts strongly to each new observation.

```{r}
#| label: fig-internet-users-forecast-non-seasonal
#| fig-cap: "Forecasting internet usage: comparing forecasting performance of non-seasonal methods."
fit %>%
  forecast(h = 10) %>%
  autoplot(www_usage) +
  labs(x="Minute", y="Number of users", title = "Internet usage per minute")
```

The resulting forecasts look sensible with decreasing trend,
    which flattens out due to the low value of the damping parameter (0.815),
    and relatively wide prediction intervals
    reflecting the variation in the historical data. </br>
The prediction intervals are calculated
    using the methods described in @sec-forecasting-ets-models.

In this example, the process of selecting a method was relatively easy
    as both MSE and MAE comparisons suggested the same method (damped Holt’s). </br>
However, sometimes different accuracy measures
    will suggest different forecasting methods,
    and then a decision is required
    as to which forecasting method we prefer to use. </br>
As forecasting tasks can vary by many dimensions
    (length of forecast horizon, size of test set,
    forecast error measures, frequency of data, etc.),
    it is unlikely that one method will be better than all others
    for all forecasting scenarios. </br>
What we require from a forecasting method are
    consistently sensible forecasts,
    and these should be frequently evaluated against the task at hand.


## Methods with seasonality {#sec-methods-with-seasonality}

Holt (1957) and Winters (1960) extended Holt’s method to capture seasonality. </br>
The Holt-Winters seasonal method comprises
    the forecast equation
    and three smoothing equations —
    one for the level $\ell_t$,
    one for the trend $b_t$, and
    one for the seasonal component $s_t$,
    with corresponding smoothing parameters $\alpha$, $\beta^*$ and $\gamma$. </br>
We use $m$ to denote the period of the seasonality, i.e.,
    the number of seasons in a year. </br>
**example**,
    for quarterly data $m = 4$, and for monthly data $m = 12$.

There are two variations to this method
    that differ in the nature of the seasonal component. </br>

**additive method**
preferred
    when the seasonal variations are roughly constant through the series. </br>
the seasonal component
    is expressed in absolute terms in the scale of the observed series, and
    in the level equation
        the series is seasonally adjusted by subtracting the seasonal component. </br>
Within each year, the seasonal component will add up to approximately zero. </br>

**multiplicative method**
preferred
    when the seasonal variations
    are changing proportional to the level of the series. </br>
the seasonal component
    is expressed in relative terms (percentages), and 
    the series is seasonally adjusted
        by dividing through by the seasonal component. </br>
Within each year, the seasonal component will sum up to approximately $m$.


### Holt-Winters’ additive method {#sec-holt-winters-additive-method}

The component form for the additive method is:
$$
\begin{align*}
    \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)} \\
    \ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\
    b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1} \\
    s_{t} &= \gamma (y_{t} - \ell_{t-1} - b_{t-1}) + (1 - \gamma)s_{t-m},
\end{align*}
$$
where
    $k$ is the integer part of $(h-1)/m$,
    which ensures that the estimates of the seasonal indices used for forecasting
    come from the final year of the sample. </br>
The level equation
    shows a weighted average between
    the seasonally adjusted observation $y_t − s_{t−m}$ and
    the non-seasonal forecast $\ell_{t−1} + b_{t−1}$ for time $t$. </br>
The trend equation is identical to **Holt’s linear method**. </br>
The seasonal equation
    shows a weighted average between
    the current seasonal index, $y_t − \ell_{t−1} − b_{t−1}$, and
    the seasonal index of the same season last year (i.e., $m$ time periods ago).

The equation for the seasonal component is often expressed as
$s_{t} = \gamma^* (y_{t} - \ell_{t}) + (1 - \gamma^*)s_{t-m}$. </br>
If we substitute $\ell_t$ from the smoothing equation
    for the level of the component form above, we get
$s_{t} = \gamma^*(1 - \alpha) (y_{t} - \ell_{t-1} - b_{t-1}) + [1 - \gamma^*(1 - \alpha)]s_{t-m},$
which is identical to
    the smoothing equation for the seasonal component we specify here,
    with $\gamma = \gamma^*(1 - \alpha)$.
The usual parameter restriction is $0 \le \gamma^* \le 1$,
    which translates to $0 \le \gamma \le 1-\alpha$


### Holt-Winters’ multiplicative method {#sec-holt-winters-multiplicative-method}

The component form for the multiplicative method is:
$$
\begin{align*}
    \hat{y}_{t+h|t} &= (\ell_{t} + hb_{t})s_{t+h-m(k+1)} \\
    \ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\
    b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1} \\
    s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}.
\end{align*}
$$


### Example: Domestic overnight trips in Australia {#sec-domestic-overnight-trips-in-australia}

We apply **Holt-Winters’ method**
    with both additive and multiplicative seasonality
    to forecast quarterly visitor nights in Australia spent by domestic tourists. </br>
@fig-australian-trips-forecast-holt-winters shows
    the data from 1998–2017,
    and the forecasts for 2018–2020. </br>
The data show an obvious seasonal pattern,
    with peaks observed in the March quarter of each year,
    corresponding to the Australian summer.

```{r}
aus_holidays <- tourism %>%
  filter(Purpose == "Holiday") %>%
  summarise(Trips = sum(Trips)/1e3)

fit <- aus_holidays %>%
  model(
    additive = ETS(Trips ~ error("A") + trend("A") + season("A")),
    multiplicative = ETS(Trips ~ error("M") + trend("A") + season("M"))
  )

fc <- fit %>% forecast(h = "3 years")


#| label: fig-australian-trips-forecast-holt-winters
#| fig-cap: "Forecasting domestic overnight trips in Australia using the Holt-Winters method with both additive and multiplicative seasonality."
fc %>%
  autoplot(aus_holidays, level = NULL) +
  labs(title="Australian domestic tourism", y="Overnight trips (millions)") +
  guides(colour = guide_legend(title = "Forecast"))


tidy(fit)
accuracy(fit)
components(fit)
```

Because both methods have exactly the same number of parameters to estimate,
    we can compare the training RMSE from both models. </br>
In this case, the method with multiplicative seasonality
    fits the data slightly better.

The estimated states for both models are plotted in
    @fig-australian-trips-forecast-holt-winters-estimated-components. </br>
The small value of $\gamma$ for the multiplicative model means that
    the seasonal component hardly changes over time. </br>
The small value of $\beta^*$ means
    the slope component hardly changes over time
    (compare the vertical scales of the slope and level components).

```{r}
#| label: fig-australian-trips-forecast-holt-winters-estimated-components
#| fig-cap: "Estimated components for the Holt-Winters method with additive and multiplicative seasonal components. "
p1 <- components(fit) %>%
  filter(.model == "additive") %>%
  autoplot() +
  labs(title = "ETS(A,A,A) decomposition", subtitle = "Additive seasonality")

p2 <- components(fit) %>%
  filter(.model == "multiplicative") %>%
  autoplot() +
  labs(title = "ETS(M,A,M) decomposition", subtitle = "Multiplicative seasonality")

p1 + p2

# overlapping plot
components(fit) %>%
  autoplot()
```


### Holt-Winters’ damped method {#sec-holt-winters-damped-method}

Damping is possible with both additive and multiplicative Holt-Winters’ methods. </br>
A method that often provides accurate and robust forecasts for seasonal data is
    the **Holt-Winters method with a damped trend and multiplicative seasonality**
$$
\begin{align*}
    \hat{y}_{t+h|t} &= \left[ \ell_{t} + (\phi + \phi^2 + \dots + \phi^{h})b_{t} \right] s_{t+h-m(k+1)} \\
    \ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1}) \\
    b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1} \\
    s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}.
\end{align*}
$$


### Example: Holt-Winters method with daily data {#sec-eg-holt-winters-method-daily data}

The Holt-Winters method can also be used for daily type of data,
    where the seasonal period is $m = 7$,
    and the appropriate unit of time for $h$ is in days. </br>
Here we forecast pedestrian traffic at a busy Melbourne train station in July 2016.

```{r}
sth_cross_ped <- pedestrian %>%
  filter(Date >= "2016-07-01", Sensor == "Southern Cross Station") %>%
  index_by(Date) %>%
  summarise(Count = sum(Count)/1000)


#| label: fig-Melbourne-daily-pedestrian-traffic-forecast
#| fig-cap: "Forecasts of daily pedestrian traffic at the Southern Cross railway station, Melbourne."
sth_cross_ped %>%
  filter(Date <= "2016-07-31") %>%
  model(hw = ETS(Count ~ error("M") + trend("Ad") + season("M"))) %>%
  forecast(h = "2 weeks") %>%
  autoplot(sth_cross_ped %>% filter(Date <= "2016-08-14")) +
  labs(title = "Daily trafficSouthern Cross", y="Pedestrians ('000)")
```

Clearly the model has identified
    the weekly seasonal pattern
    and the increasing trend at the end of the data,
    and the forecasts are a close match to the test data.


## A taxonomy of exponential smoothing methods {#sec-taxonomy-exponential-smoothing-methods}

Exponential smoothing methods
    are not restricted to those we have presented so far. </br>
By considering variations in the combinations of the trend and seasonal components,
    nine exponential smoothing methods are possible,
    listed in @tbl-classification-exponential-smoothing-methods. </br>
Each method is labelled by a pair of letters $(T,S)$
    defining the type of ‘Trend’ and ‘Seasonal’ components. </br>
**example**,
$(A,M)$ is the method with an additive trend and multiplicative seasonality;
$(Ad,N)$ is the method with damped trend and no seasonality; and so on.

| Trend Component | Seasonal Component |    |    |
| :-------------- | :----------------- | :- | :- |
| |  N | A | M |
| | (None) | (Additive) | (Multiplicative) |
| N (None) | (N,N) | (N,A) | (N,M) |
| A (Additive) | (A,N) | (A,A) | (A,M) |
| $A_d$ (Additive damped) | ($A_d$,N) | ($A_d$,A) | ($A_d$,M) |

: A two-way classification of exponential smoothing methods. {#tbl-classification-exponential-smoothing-methods}


Some of these methods we have already seen using other names:
| Short hand | Method |
| :--------- | :----- |
| (N,N) | Simple exponential smoothing |
| (A,N) | Holt’s linear method |
| ($A_d$,N) | Additive damped trend method |
| (A,A) | Additive Holt-Winters’ method |
| (A,M) | Multiplicative Holt-Winters’ method |
| ($A_d$,M) | Holt-Winters’ damped method |


@tbl-exponential-smoothing-methods-formulas
    gives the recursive formulas
    for applying the nine exponential smoothing methods in
    @tbl-classification-exponential-smoothing-methods. </br>
Each cell includes
    the forecast equation for generating $h$-step-ahead forecasts, and
    the smoothing equations for applying the method. </br>
most formulas are obtained
    by setting $\ell_t$ or $s_t$ as 0
    in $(A,A)$ and $(A,M)$ model formulas.

| Trend | Seasonal |    |    |
| :---- | :------- | :- | :- |
| |  N | A | M |
| N | $\hat{y}_{t+1 \| t} = \ell_t \\ \ell_{t} = \alpha y_{t} + (1 - \alpha)\ell_{t-1}$ | $\hat{y}_{t+h \|t} = \ell_{t} + s_{t+h-m(k+1)} \\ \ell_{t} = \alpha(y_{t} - s_{t-m}) + (1 - \alpha)\ell_{t-1} \\ s_{t} = \gamma (y_{t} - \ell_{t-1}) + (1 - \gamma)s_{t-m}$ | $\hat{y}_{t+h \|t} = \ell_{t} s_{t+h-m(k+1)} \\ \ell_{t} = \alpha(y_{t} / s_{t-m}) + (1 - \alpha)\ell_{t-1} \\ s_{t} = \gamma (y_{t} / \ell_{t-1}) + (1 - \gamma)s_{t-m}$ |
| A | $\hat{y}_{t+h \|t} = \ell_{t} + hb_{t} \\ \ell_{t} = \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\ b_{t} = \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1}$ | $\hat{y}_{t+h \|t} = \ell_{t} + hb_{t} + s_{t+h-m(k+1)} \\ \ell_{t} = \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\ b_{t} = \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1} \\ s_{t} = \gamma (y_{t} - \ell_{t-1} - b_{t-1}) + (1 - \gamma)s_{t-m}$ | $\hat{y}_{t+h \|t} = (\ell_{t} + hb_{t}) s_{t+h-m(k+1)} \\ \ell_{t} = \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\ b_{t} = \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1} \\ s_{t} = \gamma (y_{t} / (\ell_{t-1} + b_{t-1})) + (1 - \gamma)s_{t-m}$ |
| $A_d$ | $\hat{y}_{t+h \| t} = \ell_{t} + \phi_h b_{t} \\ \ell_{t} = \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1}) \\ b_{t} = \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}$ | $\hat{y}_{t+h \|t} = \ell_{t} + \phi_h b_{t} + s_{t+h-m(k+1)} \\ \ell_{t} = \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1}) \\ b_{t} = \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1} \\ s_{t} = \gamma (y_{t} - \ell_{t-1} - \phi b_{t-1}) + (1 - \gamma)s_{t-m}$ | $\hat{y}_{t+h \|t} = (\ell_{t} + \phi_h b_{t}) s_{t+h-m(k+1)} \\ \ell_{t} = \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1}) \\ b_{t} = \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1} \\ s_{t} = \gamma (y_{t} / (\ell_{t-1} + \phi b_{t-1})) + (1 - \gamma)s_{t-m}$ |

: Formulas for recursive calculations and point forecasts.
    In each case,
        $\ell_t$ denotes the series level at time $t$,
        $b_t$ denotes the slope at time $t$,
        $s_t$ denotes the seasonal component of the series at time $t$, and
        $m$ denotes the number of seasons in a year; 
        $\alpha$, $\beta^*$, $\gamma$ and $\phi$ are smoothing parameters,
        $\phi_h = \phi+\phi^2+\dots+\phi^{h}$, and
        $k$ is the integer part of $(h-1)/m$. {#tbl-exponential-smoothing-methods-formulas}


## Innovations state space models for exponential smoothing {#sec-innovations-state-space-models-exponential-smoothing}

In the rest of this chapter, we study the statistical models
    that underlie the exponential smoothing methods we have considered so far. </br>
The exponential smoothing methods presented in
    @tbl-exponential-smoothing-methods-formulas
    are algorithms which generate point forecasts. </br>
The statistical models in this section generate the same point forecasts,
    but can also generate prediction (or forecast) intervals. </br>
A statistical model is a stochastic (or random) data generating process
    that can produce an entire forecast distribution. </br>
We will also describe how to use the model selection criteria
    introduced in [Chapter 7](./chapter_7__Time_series_regression_model.qmd)
    to choose the model in an objective manner.

Each model consists of
    a measurement equation that describes
        the observed data,
    and some state equations that describe
        how the unobserved components or states (level, trend, seasonal)
    change over time. </br>
Hence, these are referred to as **state space models**.

For each method there exist two models
- one with additive errors and 
- one with multiplicative errors.
The point forecasts produced by the models are identical
    if they use the same smoothing parameter values. </br>
They will, however, generate different prediction intervals.

To distinguish between
    a model with additive errors and one with multiplicative errors
    (and also to distinguish the models from the methods),
    we add a third letter to the classification of
    @tbl-classification-exponential-smoothing-methods. </br>
We label each state space model as
    ETS($\cdot, \cdot, \cdot$) for (Error, Trend, Seasonal). </br>
This label can also be thought of as ExponenTial Smoothing. </br>
Using the same notation as in @tbl-classification-exponential-smoothing-methods,
    the possibilities for each component are
Error $=\{A,M\}$,
Trend $=\{N,A,A_d\}$ and
Seasonal $=\{N,A,M\}$


### ETS(A,N,N): simple exponential smoothing with additive errors {#sec-ets-a-n-n}

Recall the component form of simple exponential smoothing:
$$
\begin{align*}
    \text{Forecast equation} &&
        \hat{y}_{t+1|t} & = \ell_{t} \\
    \text{Smoothing equation} &&
        \ell_{t} & = \alpha y_{t} + (1 - \alpha)\ell_{t-1}.
\end{align*}
$$

If we re-arrange the smoothing equation for the level,
    we get the **error correction** form,
$$
\begin{align*}
    \ell_{t}
        &= \alpha y_{t} + \ell_{t-1} - \alpha \ell_{t-1} \\
        &= \ell_{t-1} + \alpha (y_{t} - \ell_{t-1}) \\
        &= \ell_{t-1} + \alpha e_{t},
\end{align*}
$$
where
$e_{t} = y_{t} - \ell_{t-1} = y_{t} - \hat{y}_{t|t-1}$
is the residual at time $t$.

The training data errors lead to the adjustment of the estimated level
    throughout the smoothing process for $t = 1, \dots, T$. </br>
**example**,
if the error at time $t$ is negative,
    then $y_t < \hat{y}_{t|t−1}$
    and so the level at time $t−1$ has been over-estimated. </br>
The new level $\ell_t$
    is then the previous level $\ell_{t-1}$ adjusted downwards. </br>
The closer $\alpha$ is to one,
    the “rougher” the estimate of the level
    (large adjustments take place). </br>
The smaller the $\alpha$,
    the “smoother” the level
    (small adjustments take place).

We can also write $y_t = \ell_{t−1} + e_t$,
    so that each observation can be represented by
    the previous level plus an error. </br>
To make this into an innovations state space model,
    all we need to do is specify the probability distribution for $e_t$. </br>
For a model with additive errors,
    we assume that residuals (the one-step training errors) $e_t$ are
    normally distributed white noise with mean 0 and variance $\sigma^2$. </br>
A short-hand notation for this is
    $e_t = \varepsilon_t \sim \text{NID}(0, \sigma^2)$;
    **NID** stands for **normally and independently distributed**.

Then the equations of the model can be written as
$$
y_t = \ell_{t-1} + \varepsilon_t
$$ {#eq-measurement-equation}

$$
\ell_t = \ell_{t-1} + \alpha \varepsilon_t.
$$ {#eq-state-equation}

We refer to 
    @eq-measurement-equation as the **measurement (or observation) equation** and
    @eq-state-equation as the **state (or transition) equation**. </br>
These two equations,
    together with the statistical distribution of the errors,
    form a fully specified statistical model. </br>
Specifically, these constitute an innovations state space model
    underlying simple exponential smoothing.

The term **innovations**
    comes from the fact that all equations
    use the same random error process, $\varepsilon_t$. </br>
For the same reason,
    this formulation is also referred to as a **single source of error** model. </br>
There are alternative multiple source of error formulations
    which we do not present here.

The measurement equation shows the relationship between
    the observations and the unobserved states. </br>
In this case, observation $y_t$ is a linear function of
    the level $\ell_{t−1}$,
        the predictable part of $y_t$,
    and the error $\varepsilon_t$,
        the unpredictable part of $y_t$. </br>
For other innovations state space models,
    this relationship may be nonlinear.

The state equation shows the evolution of the state through time. </br>
The influence of the smoothing parameter $\alpha$
    is the same as for the methods discussed earlier. </br>
**example**, 
$\alpha$ governs the amount of change in successive levels:
    high values of $\alpha$ allow rapid changes in the level;
    low values of $\alpha$ lead to smooth changes. </br>
If $\alpha$ = 0,
    the level of the series does not change over time;
if $\alpha$ = 1,
    the model reduces to a random walk model,
    $y_t = y_{t−1} + \varepsilon_t$.
    (See @sec-stationarity-differencing (Chapter 9) for a discussion of this model.)


### ETS(M,N,N): simple exponential smoothing with multiplicative errors {#sec-ets-m-n-n}

In a similar fashion,
    we can specify models with multiplicative errors
    by writing the one-step-ahead training errors as relative errors
$\varepsilon_t = \frac{y_t - \hat{y}_{t|t-1}}{\hat{y}_{t|t-1}}$
where
    $\varepsilon_t \sim \text{NID}(0,\sigma^2)$. </br>
Substituting
$\hat{y}_{t|t-1} = \ell_{t-1}$
gives
$y_t = \ell_{t-1} + \ell_{t-1} \varepsilon_t$ and
$e_t = y_t - \hat{y}_{t|t-1} = \ell_{t-1} \varepsilon_t$.

Then we can write the multiplicative form of the state space model as
$$
\begin{align*}
    y_t & = \ell_{t-1}(1 + \varepsilon_t) \\
    \ell_t &= \ell_{t-1}(1 + \alpha \varepsilon_t).
\end{align*}
$$


### ETS(A,A,N): Holt’s linear method with additive errors {#sec-ets-a-a-n}

For this model, we assume that the one-step-ahead training errors are given by
$\varepsilon_t = y_t - \ell_{t-1} - b_{t-1} \sim \text{NID}(0,\sigma^2)$. </br>

Substituting this into the error correction equations
    for Holt’s linear method we obtain
$$
\begin{align*}
    y_t &= \ell_{t-1} + b_{t-1} + \varepsilon_t \\
    \ell_t &= \ell_{t-1} + b_{t-1} + \alpha \varepsilon_t \\
    b_t &= b_{t-1} + \beta \varepsilon_t,
\end{align*}
$$
where
    for simplicity we have set $\beta = \alpha \beta^*$.


### ETS(M,A,N): Holt’s linear method with multiplicative errors {#sec-ets-m-a-n}

Specifying one-step-ahead training errors as relative errors such that
$\varepsilon_t = \frac{y_t - (\ell_{t-1} + b_{t-1})}{(\ell_{t-1} + b_{t-1})}$
and following an approach similar to that used above,
    the innovations state space model underlying
    Holt’s linear method with multiplicative errors is specified as
$$
\begin{align*}
    y_t &= (\ell_{t-1} + b_{t-1})(1 + \varepsilon_t) \\
    \ell_t &= (\ell_{t-1} + b_{t-1})(1 + \alpha \varepsilon_t) \\
    b_t &= b_{t-1} + \beta(\ell_{t-1} + b_{t-1}) \varepsilon_t,
\end{align*}
$$
where again
    $\beta = \alpha \beta^*$
    and $\varepsilon_t \sim \text{NID}(0,\sigma^2)$.


### Other ETS models {#sec-other-ets-models}

In a similar fashion, we can write an innovations state space model
    for each of the exponential smoothing methods of
    @tbl-exponential-smoothing-methods-formulas. </br>
@tbl-state-space-equations-additive-error-models and
    @tbl-state-space-equations-multiplicative-error-models
    presents the equations for all of the models in the ETS framework.


| Trend | Seasonal |    |    |
| :---- | :------- | :- | :- |
| |  N | A | M |
| N | $y_t = \ell_{t-1} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + \alpha \varepsilon_{t}$ | $y_t = \ell_{t-1} + s_{t-m} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + \alpha \varepsilon_{t} \\ s_{t} = s_{t-m} + \gamma \varepsilon_{t}$ | $y_t = \ell_{t-1} s_{t-m} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + \alpha \varepsilon_{t} / s_{t-m} \\ s_{t} = s_{t-m} + \gamma \varepsilon_{t} / \ell_{t-1}$ |
| A | $y_t = \ell_{t-1} + b_{t-1} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + b_{t-1} + \alpha \varepsilon_{t} \\ b_{t} = b_{t-1} + \beta \varepsilon_{t}$ | $y_t = \ell_{t-1} + b_{t-1} + s_{t-m} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + b_{t-1} + \alpha \varepsilon_{t} \\ b_{t} = b_{t-1} + \beta \varepsilon_{t} \\ s_{t} = s_{t-m} + \gamma \varepsilon_{t}$ | $y_t = (\ell_{t-1} + b_{t-1}) s_{t-m} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + b_{t-1} + \alpha \varepsilon_{t} / s_{t-m} \\ b_{t} = b_{t-1} + \beta \varepsilon_{t} / s_{t-m} \\ s_{t} = s_{t-m} + \gamma \varepsilon_{t} / (\ell_{t-1} + b_{t-1})$ |
| $A_d$ | $y_t = \ell_{t-1} + \phi b_{t-1} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + \phi b_{t-1} + \alpha \varepsilon_{t} \\ b_{t} = \phi b_{t-1} + \beta \varepsilon_{t}$ | $y_t = \ell_{t-1} + \phi b_{t-1} + s_{t-m} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + \phi b_{t-1} + \alpha \varepsilon_{t} \\ b_{t} = \phi b_{t-1} + \beta \varepsilon_{t} \\ s_{t} = s_{t-m} + \gamma \varepsilon_{t}$ | $y_t = (\ell_{t-1} + \phi b_{t-1}) s_{t-m} + \varepsilon_{t} \\ \ell_{t} = \ell_{t-1} + \phi b_{t-1} + \alpha \varepsilon_{t} / s_{t-m} \\ b_{t} = \phi b_{t-1} + \beta \varepsilon_{t} / s_{t-m} \\ s_{t} = s_{t-m} + \gamma \varepsilon_{t} / (\ell_{t-1} + \phi b_{t-1})$ |

: State space equations for each of the additive models in the ETS framework. {#tbl-state-space-equations-additive-error-models}



| Trend | Seasonal |    |    |
| :---- | :------- | :- | :- |
| |  N | A | M |
| N | $y_t = \ell_{t-1} (1 + \varepsilon_{t}) \\ \ell_{t} = \ell_{t-1} (1 + \alpha \varepsilon_{t})$ | $y_t = (\ell_{t-1} + s_{t-m}) (1 + \varepsilon_{t}) \\ \ell_{t} = \ell_{t-1} + \alpha (\ell_{t-1} + s_{t-m}) \varepsilon_{t} \\ s_{t} = s_{t-m} + \gamma (\ell_{t-1} + s_{t-m}) \varepsilon_{t}$ | $y_t = \ell_{t-1} s_{t-m} (1 + \varepsilon_{t}) \\ \ell_{t} = \ell_{t-1} (1 + \alpha \varepsilon_{t}) \\ s_{t} = s_{t-m} (1 + \gamma \varepsilon_{t})$ |
| A | $y_t = (\ell_{t-1} + b_{t-1}) (1 + \varepsilon_{t}) \\ \ell_{t} = (\ell_{t-1} + b_{t-1}) (1 + \alpha \varepsilon_{t}) \\ b_{t} = b_{t-1} + \beta (\ell_{t-1} + b_{t-1}) \varepsilon_{t}$ | $y_t = (\ell_{t-1} + b_{t-1} + s_{t-m}) (1 + \varepsilon_{t}) \\ \ell_{t} = \ell_{t-1} + b_{t-1} + \alpha (\ell_{t-1} + b_{t-1} + s_{t-m}) \varepsilon_{t} \\ b_{t} = b_{t-1} + \beta (\ell_{t-1} + b_{t-1} + s_{t-m}) \varepsilon_{t} \\ s_{t} = s_{t-m} + \gamma (\ell_{t-1} + b_{t-1} + s_{t-m}) \varepsilon_{t}$ | $y_t = (\ell_{t-1} + b_{t-1}) s_{t-m} (1 + \varepsilon_{t}) \\ \ell_{t} = (\ell_{t-1} + b_{t-1}) (1 + \alpha \varepsilon_{t}) \\ b_{t} = b_{t-1} + \beta (\ell_{t-1} + b_{t-1}) \varepsilon_{t} \\ s_{t} = s_{t-m} (1 + \gamma \varepsilon_{t})$ |
| $A_d$ | $y_t = (\ell_{t-1} + \phi b_{t-1}) (1 + \varepsilon_{t}) \\ \ell_{t} = (\ell_{t-1} + \phi b_{t-1}) (1 + \alpha \varepsilon_{t}) \\ b_{t} = \phi b_{t-1} + \beta (\ell_{t-1} + \phi b_{t-1}) \varepsilon_{t}$ | $y_t = (\ell_{t-1} + \phi b_{t-1} + s_{t-m}) (1 + \varepsilon_{t}) \\ \ell_{t} = \ell_{t-1} + \phi b_{t-1} + \alpha (\ell_{t-1} + \phi b_{t-1} + s_{t-m}) \varepsilon_{t} \\ b_{t} = \phi b_{t-1} + \beta (\ell_{t-1} + \phi b_{t-1} + s_{t-m}) \varepsilon_{t} \\ s_{t} = s_{t-m} + \gamma (\ell_{t-1} + \phi b_{t-1} + s_{t-m}) \varepsilon_{t}$ | $y_t = (\ell_{t-1} + \phi b_{t-1}) s_{t-m} (1 + \varepsilon_{t}) \\ \ell_{t} = (\ell_{t-1} + \phi b_{t-1}) (1 + \alpha \varepsilon_{t}) \\ b_{t} = \phi b_{t-1} + \beta (\ell_{t-1} + \phi b_{t-1}) \varepsilon_{t} \\ s_{t} = s_{t-m} (1 + \gamma \varepsilon_{t})$ |

: State space equations for each of the multiplicative models in the ETS framework. {#tbl-state-space-equations-multiplicative-error-models}


## Estimation and model selection {#sec-estimation-model-selection}


### Estimating ETS models {#sec-estimating-ets-models}

An alternative to estimating the parameters by minimising the sum of squared errors
    is to maximise the “likelihood.” </br>

> **likelihood** </br>
    is the probability of the data arising from the specified model. </br>

Thus, a large likelihood is associated with a good model. </br>
For an additive error model,
    maximising the likelihood (assuming normally distributed errors)
    gives the same results as minimising the sum of squared errors. </br>
However, different results will be obtained for multiplicative error models. </br>
In this section, we will estimate
    the smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$,
    and the initial states $\ell_0$, $b_0$, $s_0, s_{-1}, \dots, s_{-m+1}$,
    by maximising the likelihood.

The possible values that the smoothing parameters can take are restricted. </br>
Traditionally, the parameters have been constrained to lie between 0 and 1
    so that the equations can be interpreted as weighted averages. </br>
That is, $0 < \alpha, \beta^*, \gamma^∗, \phi < 1$. </br>
For the state space models, we have set
    $\beta = \alpha \beta^*$ and
    $\gamma = (1 - \alpha) \gamma^*$. </br>
Therefore, the traditional restrictions translate to
    $0 < \alpha < 1$,
    $0 < \beta < \alpha$ and
    $0 < \gamma < 1−\alpha$. </br>
In practice, the damping parameter $\phi$ is usually constrained further
    to prevent numerical difficulties in estimating the model. </br>
In the `fable` package, it is restricted so that
    $0.8 < \phi < 0.98$.

Another way to view the parameters
    is through a consideration of the mathematical properties
    of the state space models. </br>
The parameters are constrained
    in order to prevent observations in the distant past
    having a continuing effect on current forecasts. </br>
This leads to some admissibility constraints on the parameters,
    which are usually
    (but not always)
    less restrictive than the traditional constraints region
    (Hyndman et al., 2008, p. Ch10). </br>
**example**, 
for the ETS(A,N,N) model,
    the traditional parameter region is $0 < \alpha < 1$ but
    the admissible region is $0 < \alpha < 2$. </br>
For the ETS(A,A,N) model,
    the traditional parameter region is
        $0 < \alpha < 1$ and
        $0 < \beta < \alpha$ but 
    the admissible region is
        $0 < \alpha < 2$ and
        $0 < \beta < 4−2\alpha$.


### Model selection {#sec-model-selection}

A great advantage of the ETS statistical framework is that
    information criteria can be used for model selection. </br>
The AIC, $\text{AIC}_\text{c}$ and BIC, introduced in @sec-selecting-predictors,
    can be used here to determine
    which of the ETS models is most appropriate for a given time series.

For ETS models, **Akaike’s Information Criterion (AIC)** is defined as
    $ \text{AIC} = -2\log(L) + 2k$,
where
    $L$ is the likelihood of the model and 
    $k$ is the total number of parameters and initial states
        that have been estimated (including the residual variance).

The **AIC corrected for small sample bias ($\text{AIC}_{\text{c}}$)** is defined as
$\text{AIC}_{\text{c}} = \text{AIC} + \frac{2k(k+1)}{T-k-1}$

**Bayesian Information Criterion (BIC)** is 
$ \text{BIC} = \text{AIC} + k[\log(T)-2]$

Three of the combinations of (Error, Trend, Seasonal)
    can lead to numerical difficulties. </br>
Specifically, the models that can cause such instabilities are
    ETS(A,N,M), ETS(A,A,M), and ETS(A,$A_d$,M),
    due to division by values potentially close to zero in the state equations. </br>
We normally do not consider these particular combinations when selecting a model.

Models with multiplicative errors are useful when the data are strictly positive,
    but are not numerically stable when the data contain zeros or negative values. </br>
Therefore, multiplicative error models will not be considered
    if the time series is not strictly positive. </br>
In that case, only the six fully additive models will be applied.


### Example: Domestic holiday tourist visitor nights in Australia {#sec-australia-tourist-visitor}

We let the `ETS()` function select the model by minimising the AICc.

```{r}
aus_holidays <- tourism %>%
  filter(Purpose == "Holiday") %>%
  summarise(Trips = sum(Trips)/1e3)

fit <- aus_holidays %>%
  model(ETS(Trips))

report(fit)
```

The model selected is ETS(M,N,A) 
$$
\begin{align*}
    y_{t} &= (\ell_{t-1} + s_{t-m})(1 + \varepsilon_t) \\
    \ell_t &= \ell_{t-1} + \alpha(\ell_{t-1} + s_{t-m})\varepsilon_t \\
    s_t &= s_{t-m} + \gamma(\ell_{t-1} + s_{t-m}) \varepsilon_t.
\end{align*}
$$

The parameter estimates are $\hat{\alpha} = 0.3484$, and $\hat{\gamma} = 0.0001$. </br>
The output also returns the estimates for the initial states
    $\ell_0$, $s_0$, $s_{−1}$, $s_{−2}$ and $s_{−3}$. </br>
Compare these with the values obtained for
    the Holt-Winters method with additive seasonality
    presented in @sec-domestic-overnight-trips-in-australia.

@fig-mna-states shows the states over time, while
    @fig-mna-forecasts
        shows point forecasts and prediction intervals generated from the model. </br>
The small values of $\gamma$ indicate that
    the seasonal components change very little over time.

```{r}
#| label: fig-mna-states.
#| fig-cap: "Graphical representation of the estimated states over time."
components(fit) %>%
  autoplot() +
  labs(title = "ETS(M,N,A) components")
```

Because this model has multiplicative errors,
    the innovation residuals are not equivalent to the regular residuals
    (i.e., the one-step training errors). </br>
The innovation residuals are given by $\hat{\varepsilon}_t$,
    while the regular residuals are defined as $y_t − \hat{y}_{t|t−1}$. </br>
We can obtain both using the `augment()` function. </br>
They are plotted in @fig-mna-residuals.

```{r}
#| label: fig-mna-residuals
#| fig-cap: "Residuals and one-step forecast errors from the ETS(M,N,A) model."
augment(fit) %>%
  select(Quarter, .resid, .innov) %>%
  pivot_longer(-Quarter) %>%
  autoplot(value) +
  facet_wrap(vars(name), scales = "free_y", dir = "v", strip.position = "right") +
  theme(legend.position = "none")
```


## Forecasting with ETS models {#sec-forecasting-ets-models}

Point forecasts can be obtained from the models
    by iterating the equations 
    for $t = T+1, \dots, T+h$
    and setting all $\varepsilon_t = 0$ for $t > T$.

For **example**,
    for model ETS(M,A,N),
    $y_{T+1} = (\ell_T + b_T)(1 + \varepsilon_{T+1})$. </br>
Therefore
    $\hat{y}_{T+1|T} = \ell_T + b_T$. </br>
Similarly,
$$
\begin{align*}
    y_{T+2}
        &= (\ell_{T+1} + b_{T+1})(1 + \varepsilon_{T+2}) \\
        &= \left[ 
            (\ell_T + b_T) (1 + \alpha \varepsilon_{T+1})
            + b_T
            + \beta (\ell_T + b_T)\varepsilon_{T+1}
        \right] (1 + \varepsilon_{T+2}).
\end{align*}
$$

Therefore, $\hat{y}_{T+2|T} = \ell_T + 2b_T$, and so on. </br>
These forecasts are identical to the forecasts from Holt’s linear method,
    and also to those from model ETS(A,A,N). </br>
Thus, the point forecasts obtained from the method
    and from the two models that underlie the method
    are identical
    (assuming that the same parameter values are used). </br>
ETS point forecasts constructed in this way are equal to
    the means of the forecast distributions,
    except for the models with multiplicative seasonality (Hyndman et al., 2008).

To obtain forecasts from an ETS model,
    we use the `forecast()` function from the `fable` package. </br>
This function will always return the means of the forecast distribution,
    even when they differ from these traditional point forecasts.

```{r}
#| label: fig-mna-forecasts
#| fig-cap: "Forecasting Australian domestic overnight trips using an ETS(M,N,A) model."
Figure 3
fit %>%
  forecast(h = 8) %>%
  autoplot(aus_holidays)+
  labs(title = "Australian domestic tourism", y = "Overnight trips (millions)")
```


### Prediction intervals {#sec-prediction-intervals}

A big advantage of the models is that
    prediction intervals can also be generated —
    something that cannot be done using the methods. </br>
The prediction intervals will differ
    between models with additive and multiplicative methods.

For most ETS models, a prediction interval can be written as
$\hat{y}_{T+h|T} \pm c \sigma_h$
where 
    $c$ depends on the coverage probability, and 
    $\sigma_h^2$ is the forecast variance. </br>

For ETS models,
    formulas for $\sigma_h^2$ can be complicated;
    the details are given in Chapter 6 of Hyndman et al. (2008). </br>
In @tbl-model-forecast-variance
    we give the formulas for the additive ETS models, which are the simplest.

| Model | Forecast variance: $\sigma_h^2$ |
| :---- | :------------------------------ |
| (A,N,N) | $\sigma_h^2 = \sigma^2\big[1 + \alpha^2(h-1)\big]$ |
| (A,A,N) | $\sigma_h^2 = \sigma^2\Big[1 + (h-1)\big\{\alpha^2 + \alpha\beta h + \frac16\beta^2h(2h-1)\big\}\Big]$ |
| (A,$A_d$,N) | $\sigma_h^2 = \sigma^2\biggl[1 + \alpha^2(h-1) + \frac{\beta\phi h}{(1-\phi)^2} \left\{2\alpha(1-\phi) +\beta\phi\right\} - \frac{\beta\phi(1-\phi^h)}{(1-\phi)^2(1-\phi^2)} \left\{ 2\alpha(1-\phi^2)+ \beta\phi(1+2\phi-\phi^h)\right\}\biggr]$ |
| (A,N,A) | $\sigma_h^2 = \sigma^2\Big[1 + \alpha^2(h-1) + \gamma k(2\alpha+\gamma)\Big]$ |
| (A,A,A) | $\sigma_h^2 = \sigma^2\Big[1 + (h-1)\big\{\alpha^2 + \alpha\beta h + \frac16\beta^2h(2h-1)\big\} + \gamma k \big\{2\alpha+ \gamma + \beta m (k+1)\big\} \Big]$ |
| (A,$A_d$,A) | $\sigma_h^2 = \sigma^2\biggl[1 + \alpha^2(h-1) + \gamma k(2\alpha+\gamma) +\frac{\beta\phi h}{(1-\phi)^2} \left\{2\alpha(1-\phi) + \beta\phi \right\} - \frac{\beta\phi(1-\phi^h)}{(1-\phi)^2(1-\phi^2)} \left\{ 2\alpha(1-\phi^2)+ \beta\phi(1+2\phi-\phi^h)\right\} + \frac{2\beta\gamma\phi}{(1-\phi)(1-\phi^m)}\left\{k(1-\phi^m) - \phi^m(1-\phi^{mk})\right\}\biggr]$ |

: Forecast variance expressions for each additive state space model,
    where $\sigma^2$ is the residual variance,
    $m$ is the seasonal period, and
    $k$ is the integer part of $(h-1) / m$
        (i.e., the number of complete years
        in the forecast period prior to time $T+h$). {#tbl-model-forecast-variance}

For a few ETS models, there are no known formulas for prediction intervals. </br>
In these cases, the `forecast()` function
    uses simulated future sample paths
    and computes prediction intervals
    from the percentiles of these simulated future paths.


## Exercises


## Future Reading
