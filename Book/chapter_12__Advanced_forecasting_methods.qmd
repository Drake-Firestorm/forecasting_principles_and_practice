---
title: "Advanced forecasting methods"
format:
  html:
    code-fold: true
number-sections: true
---

In this chapter, we briefly discuss
    four more advanced forecasting methods
    that build on the models discussed in earlier chapters.


## Complex seasonality {#sec-complex-seasonality}

So far, we have mostly considered relatively simple seasonal patterns
    such as quarterly and monthly data. </br>
However, higher frequency time series
    often exhibit more complicated seasonal patterns. </br>
**example**,
    daily data may have a weekly pattern as well as an annual pattern. </br>
Hourly data usually has three types of seasonality:
    a daily pattern,
    a weekly pattern, and
    an annual pattern. </br>
Even weekly data can be challenging to forecast
    as there are not a whole number of weeks in a year,
    so the annual pattern has a seasonal period of
    $365.25/7 \approx 52.179$ on average. </br>
Most of the methods we have considered so far
    are unable to deal with these seasonal complexities.

We don’t necessarily want to include
    all of the possible seasonal periods in our models —
    just the ones that are likely to be present in the data. </br>
**example**,
    if we have only 180 days of data, we may ignore the annual seasonality. </br>
If the data are measurements of a natural phenomenon (e.g., temperature),
    we can probably safely ignore any weekly seasonality.

@fig-call-volume shows
    the number of calls to a North American commercial bank
    per 5-minute interval between 7:00am and 9:05pm each weekday
    over a 33 week period. </br>
The lower panel shows the first four weeks of the same time series. </br>
There is a strong daily seasonal pattern with period 169
    (there are 169 5-minute intervals per day),
    and a weak weekly seasonal pattern with period $169 \times 5 = 845$.
    (Call volumes on Mondays tend to be higher than the rest of the week.) </br>
If a longer series of data were available,
    we may also have observed an annual seasonal pattern.
```{r}
#| label: fig-call-volume
#| fig-cap: "Five-minute call volume handled on weekdays between 7:00am and 9:05pm in a large North American commercial bank. Top panel: data from 3 March – 24 October 2003. Bottom panel: first four weeks of data."
p1 <- bank_calls %>%
  fill_gaps() %>%
  autoplot(Calls) +
  labs(y = "Calls", title = "Five-minute call volume to bank")

p2 <- bank_calls %>%
  fill_gaps() %>%
  filter(DateTime <= min(DateTime) + dweeks(4)) %>%
  autoplot(Calls) +
  labs(y = "Calls", title = "Five-minute call volume over 4 weeks")

p1 / p2
```

Apart from the multiple seasonal periods,
    this series has the additional complexity
    of missing values between the working periods.


### STL with multiple seasonal periods {#sec-stl-multiple-seasonal-periods}

The `STL()` function is designed to deal with multiple seasonality. </br>
It will return multiple seasonal components,
    as well as a trend and remainder component. </br>
In this case,
    we need to re-index the tsibble to avoid the missing values,
    and then explicitly give the seasonal periods.
```{r}
#| label: fig-call-volume-stl-decomposition
#| fig-cap: "STL decomposition with multiple seasonality for the call volume data."
calls <- bank_calls %>%
  mutate(t = row_number()) %>%
  update_tsibble(index = t, regular = TRUE)

calls %>%
  model(
    STL(sqrt(Calls) ~ season(period = 169) + season(period = 5*169), robust = TRUE)
  ) %>%
  components() %>%
  autoplot() + labs(x = "Observation")
```

There are two seasonal patterns shown,
    one for the time of day (the third panel),
    and one for the time of week (the fourth panel). </br>
To properly interpret this graph,
    it is important to notice the vertical scales. </br>
In this case, the trend and the weekly seasonality have wider bars
    (and therefore relatively narrower ranges)
    compared to the other components,
    because there is little trend seen in the data,
    and the weekly seasonality is weak.

The decomposition can also be used in forecasting,
    with each of the seasonal components forecast using a seasonal naïve method,
    and the seasonally adjusted data forecast using ETS.

The code is slightly more complicated than usual
    because we have to add back the time stamps that were lost
    when we re-indexed the tsibble
    to handle the periods of missing observations. </br>
The square root transformation used in the STL decomposition
    has ensured the forecasts remain positive.

```{r}
#| label: fig-call-volume-forecast
#| fig-cap: "Forecasts of the call volume data using an STL decomposition with the seasonal components forecast using a seasonal naïve method, and the seasonally adjusted data forecast using ETS."

# Forecasts from STL+ETS decomposition
my_dcmp_spec <- decomposition_model(
  STL(sqrt(Calls) ~ season(period = 169) + season(period = 5*169), robust = TRUE),
  ETS(season_adjust ~ season("N"))
)

fc <- calls %>%
  model(my_dcmp_spec) %>%
  forecast(h = 5 * 169)


# Add correct time stamps to fable
fc_with_times <- bank_calls %>%
  new_data(n = 7 * 24 * 60 / 5) %>%
  mutate(time = format(DateTime, format = "%H:%M:%S")) %>%
  filter(
    time %in% format(bank_calls$DateTime, format = "%H:%M:%S"),
    wday(DateTime, week_start = 1) <= 5
  ) %>%
  mutate(t = row_number() + max(calls$t)) %>%
  left_join(fc, by = "t") %>%
  as_fable(response = "Calls", distribution = Calls)


# Plot results with last 3 weeks of data
fc_with_times %>%
  fill_gaps() %>%
  autoplot(bank_calls %>% tail(14 * 169) %>% fill_gaps()) +
  labs(y = "Calls", title = "Five-minute call volume to bank")
```

### Dynamic harmonic regression with multiple seasonal periods {#sec-dynamic-harmonic-regression-multiple-seasonal-periods}

With multiple seasonalities,
    we can use Fourier terms as we did in earlier chapters
    (see @sec-some-useful-predictors (Chaptr 7)
        and @sec-dynamic-harmonic-regression (Chapter 10)). </br>
Because there are multiple seasonalities,
    we need to add Fourier terms for each seasonal period. </br>
In this case, the seasonal periods are 169 and 845,
    so the Fourier terms are of the form
$\sin\left(\frac{2\pi kt}{169}\right)
    , \quad \cos\left(\frac{2\pi kt}{169}\right)
    , \quad \sin\left(\frac{2\pi kt}{845}\right)
    , \quad \text{and} \quad
    \cos\left(\frac{2\pi kt}{845}\right),$
for
    $k = 1, 2, \dots$. </br>
As usual, the `fourier()` function can generate these for you.

We will fit a dynamic harmonic regression model
    with an ARIMA error structure. </br>
The total number of Fourier terms for each seasonal period
    could be selected to minimise the AICc. </br>
However, for high seasonal periods,
    this tends to over-estimate the number of terms required,
    so we will use a more subjective choice with
    10 terms for the daily seasonality and
    5 for the weekly seasonality. </br>
Again, we will use a square root transformation
    to ensure the forecasts and prediction intervals remain positive. </br>
We set $D=d=0$ in order to handle the non-stationarity through the regression terms,
    and $P=Q=0$ in order to handle the seasonality through the regression terms.
```{r}
#| label: fig-call-volume-forecasts
#| fig-cap: "Forecasts from a dynamic harmonic regression applied to the call volume data."
fit <- calls %>%
  model(
    dhr = ARIMA(sqrt(Calls) ~ PDQ(0, 0, 0) + pdq(d = 0) +
                  fourier(period = 169, K = 10) +
                  fourier(period = 5*169, K = 5))
  )

fc <- fit %>% forecast(h = 5 * 169)


# Add correct time stamps to fable
fc_with_times <- bank_calls %>%
  new_data(n = 7 * 24 * 60 / 5) %>%
  mutate(time = format(DateTime, format = "%H:%M:%S")) %>%
  filter(
    time %in% format(bank_calls$DateTime, format = "%H:%M:%S"),
    wday(DateTime, week_start = 1) <= 5
  ) %>%
  mutate(t = row_number() + max(calls$t)) %>%
  left_join(fc, by = "t") %>%
  as_fable(response = "Calls", distribution = Calls)


# Plot results with last 3 weeks of data
fc_with_times %>%
  fill_gaps() %>%
  autoplot(bank_calls %>% tail(14 * 169) %>% fill_gaps()) +
  labs(y = "Calls", title = "Five-minute call volume to bank")
```

This is a large model, containing 33 parameters:
    4 ARMA coefficients,
    20 Fourier coefficients for period 169,
    and 8 Fourier coefficients for period 845. </br>
Not all of the Fourier terms for period 845 are used
    because there is some overlap with the terms of period 169
    (since $845 = 5 \times 169$).


### Example: Electricity demand {#sec-eg-electricity-demand-harmonic-regression}

One common application of such models is electricity demand modelling. </br>
@fig-electricity-demand shows
    half-hourly electricity demand (MWh) in Victoria, Australia, during 2012–2014,
    along with temperatures (degrees Celsius) for the same period for
    Melbourne (the largest city in Victoria).
```{r}
#| label: fig-electricity-demand
#| fig-cap: "Half-hourly electricity demand and corresponding temperatures in 2012–2014, Victoria, Australia."
vic_elec %>%
  pivot_longer(Demand:Temperature, names_to = "Series") %>%
  ggplot(aes(x = Time, y = value)) +
  geom_line() +
  facet_grid(rows = vars(Series), scales = "free_y") +
  labs(y = "")
```

Plotting electricity demand against temperature
    (@fig-electricity-demand-temperatures) shows
    that there is a nonlinear relationship between the two,
    with demand increasing for low temperatures (due to heating)
    and increasing for high temperatures (due to cooling).
```{r}
#| label: fig-electricity-demand-temperatures
#| fig-cap: "Half-hourly electricity demand for Victoria, plotted against temperatures for the same times in Melbourne, the largest city in Victoria."
elec <- vic_elec %>%
  mutate(
    DOW = wday(Date, label = TRUE),
    Working_Day = !Holiday & !(DOW %in% c("Sat", "Sun")),
    Cooling = pmax(Temperature, 18)
  )

elec %>%
  ggplot(aes(x=Temperature, y=Demand, col=Working_Day)) +
  geom_point(alpha = 0.6) +
  labs(x="Temperature (degrees Celsius)", y="Demand (MWh)")
```

We will fit a regression model
    with a piecewise linear function of temperature
    (containing a knot at 18 degrees),
    and harmonic regression terms to allow for the daily seasonal pattern. </br>
Again, we set the orders of the Fourier terms subjectively,
    while using the AICc to select the order of the ARIMA errors.
```{r}
fit <- elec %>%
  model(
    ARIMA(Demand ~ PDQ(0, 0, 0) + pdq(d = 0) +
            Temperature + Cooling + Working_Day +
            fourier(period = "day", K = 10) +
            fourier(period = "week", K = 5) +
            fourier(period = "year", K = 3))
  )
```

Forecasting with such models is difficult
    because we require future values of the predictor variables. </br>
Future values of the Fourier terms are easy to compute,
    but future temperatures are, of course, unknown. </br>
If we are only interested in forecasting up to a week ahead,
    we could use temperature forecasts obtain from a meteorological model. </br>
Alternatively, we could use scenario forecasting
    (@sec-scenario-forecasting (Chapter 6))
    and plug in possible temperature patterns. </br>
In the following example,
    we have used a repeat of the last two days of temperatures
    to generate future possible demand values.
```{r}
#| label: fig-electricity-forecasts-demand-harmonic-regression
#| fig-cap: "Forecasts from a dynamic harmonic regression model applied to half-hourly electricity demand data."
elec_newdata <- new_data(elec, 2*48) %>%
  mutate(
    Temperature = tail(elec$Temperature, 2 * 48),
    Date = lubridate::as_date(Time),
    DOW = wday(Date, label = TRUE),
    Working_Day = (Date != "2015-01-01") & !(DOW %in% c("Sat", "Sun")),
    Cooling = pmax(Temperature, 18)
  )

fc <- fit %>%
  forecast(new_data = elec_newdata)

fc %>%
  autoplot(elec %>% tail(10 * 48)) +
  labs(title="Half hourly electricity demand: Victoria", y = "Demand (MWh)", x = "Time [30m]")
```

Although the short-term forecasts look reasonable,
    this is a crude model for a complicated process. </br>
The residuals, plotted in @fig-electricity-demand-harmonic-regression-residual,
    demonstrate that there is a lot of information
    that has not been captured with this model. </br>
```{r}
#| label: fig-electricity-demand-harmonic-regression-residual
#| fig-cap: "Residual diagnostics for the dynamic harmonic regression model."
fit %>% gg_tsresiduals()
```

More sophisticated versions of this model which provide much better forecasts
    are described in Hyndman & Fan (2010) and Fan & Hyndman (2012).


## Prophet model {#sec-prophet-model}

A recent proposal is the Prophet model,
    available via the `fable.prophet` package. </br>
This model was introduced by Facebook (S. J. Taylor & Letham, 2018),
    originally for forecasting daily data with weekly and yearly seasonality,
    plus holiday effects. </br>
It was later extended to cover more types of seasonal data. </br>
It works best with time series
    that have strong seasonality and several seasons of historical data.

Prophet can be considered a nonlinear regression model
    ([Chapter 7](./chapter_7__Time_series_regression_model.qmd)), of the form
    $y_t = g(t) + s(t) + h(t) + \varepsilon_t,$
where
    $g(t)$ describes a piecewise-linear trend (or “growth term”),
    $s(t)$ describes the various seasonal patterns,
    $h(t)$ captures the holiday effects, and
    $\varepsilon_t$ is a white noise error term.

- The knots (or changepoints) for the piecewise-linear trend
    are automatically selected if not explicitly specified.
    Optionally, a logistic function can be used to set an upper bound on the trend.
- The seasonal component consists of Fourier terms of the relevant periods.
    By default,
        order 10 is used for annual seasonality and
        order 3 is used for weekly seasonality.
- Holiday effects are added as simple dummy variables.
- The model is estimated using a Bayesian approach
    to allow for automatic selection of the changepoints
    and other model characteristics.

We illustrate the approach using two data sets:
    a simple quarterly example,
    and then the electricity demand data described in the previous section.


### Example: Quarterly cement production {#sec-eg-quarterly-cement-production}

For the simple quarterly example,
    we will repeat the analysis from @sec-arima-vs-ets
    in which we compared an ARIMA and ETS model,
    but we will add in a prophet model for comparison.
```{r}
library(fable.prophet)

cement <- aus_production %>%
  filter(year(Quarter) >= 1988)

train <- cement %>%
  filter(year(Quarter) <= 2007)

fit <- train %>%
  model(
    arima = ARIMA(Cement),
    ets = ETS(Cement),
    prophet = prophet(Cement ~ season(period = 4, order = 2, type = "multiplicative"))
  )
```

Note that the seasonal term
    must have the `period` fully specified for quarterly and monthly data,
    as the default values assume the data are observed at least daily.
```{r}
#| label: fig-cement-production-forecast
#| fig-cap: "Prophet compared to ETS and ARIMA on the Cement production data, with a 10-quarter test set."
fc <- fit %>% forecast(h = "2 years 6 months")

fc %>% autoplot(cement)
```

In this example, the Prophet forecasts are worse
    than either the ETS or ARIMA forecasts.
```{r}
fc %>% accuracy(cement)
```


### Example: Half-hourly electricity demand {#sec-eg-electricity-demand-prophet}

We will fit a similar model to
    the dynamic harmonic regression (DHR) model from the previous section,
    but this time using a Prophet model. </br>
For daily and sub-daily data, the default periods are correctly specified,
    so that we can simply specify the period using a character string as follows.
```{r}
#| label: fig-electricity-demand-prophet
#| fig-cap: "Components of a Prophet model fitted to the Victorian electricity demand data."
fit <- elec %>%
  model(
    prophet(Demand ~ Temperature + Cooling + Working_Day +
              season(period = "day", order = 10) +
              season(period = "week", order = 5) +
              season(period = "year", order = 3))
  )

fit %>%
  components() %>%
  autoplot()
```

@fig-electricity-demand-prophet shows
    the trend and seasonal components of the fitted model.

The model specification is very similar to the DHR model in the previous section,
    although the result is different in several important ways. </br>
The Prophet model adds a piecewise linear time trend
    which is not really appropriate here
    as we don’t expect the long term forecasts to continue to follow
    the downward linear trend at the end of the series.

There is also substantial remaining autocorrelation in the residuals,
```{r}
#| label: fig-electricity-demand-prophet-residuals
#| fig-cap: "Residuals from the Prophet model for Victorian electricity demand."
fit %>% gg_tsresiduals()
```

As a result, the prediction intervals are probably too narrow.
```{r}
#| label: fig-electricity-demand-forecasts-prophet
#| fig-cap: "Two day forecasts from the Prophet model for Victorian electricity demand."
fc <- fit %>%
  forecast(new_data = elec_newdata)

fc %>%
  autoplot(elec %>% tail(10 * 48)) +
  labs(x = "Date", y = "Demand (MWh)")
```

Prophet has the advantage of being much faster to estimate
    than the DHR models we have considered previously,
    and it is completely automated. </br>
However, it rarely gives better forecast accuracy than the alternative approaches,
    as these two examples have illustrated.


## Vector autoregressions {#sec-vector-autoregressions}

One limitation of the models that we have considered so far
    is that they impose a *unidirectional relationship* —
    the forecast variable is influenced by the predictor variables,
    but not vice versa. </br>
However, there are many cases where the reverse should also be allowed for —
    where all variables affect each other. </br>
In @sec-regression-with-arima-errors-using-fable (Chapter 10),
    the changes in personal consumption expenditure ($C_t$) were forecast
    based on the changes in personal disposable income ($I_t$). </br>
However, in this case a bi-directional relationship may be more suitable:
    an increase in $I_t$ will lead to an increase in $C_t$ and vice versa.

**example**
    of such a situation occurred in Australia
    during the Global Financial Crisis of 2008–2009. </br>
The Australian government issued stimulus packages
    that included cash payments in December 2008,
    just in time for Christmas spending. </br>
As a result, retailers reported strong sales and the economy was stimulated. </br>
Consequently, incomes increased.

Such feedback relationships are allowed for
    in the **vector autoregressive (VAR) framework**. </br>
In this framework, all variables are treated symmetrically. </br>
They are all modelled as if they all influence each other equally. </br>
In more formal terminology,
    all variables are now treated as **endogenous**. </br>
To signify this, we now change the notation and write all variables as $y$'s:
    $y_{1,t}$ denotes the $t$ th observation of variable $y_1$,
    $y_{2,t}$ denotes the $t$ th observation of variable $y_2$, and so on.

A VAR model is a generalisation of the univariate autoregressive model
    for forecasting a vector of time series. </br>
A more flexible generalisation would be a **Vector ARMA** process. </br>
However, the relative simplicity of VARs
    has led to their dominance in forecasting. </br>
It comprises one equation per variable in the system. </br>
The right hand side of each equation includes a constant
    and lags of all of the variables in the system. </br>
To keep it simple, we will consider a two variable VAR with one lag. </br>
We write a 2-dimensional VAR(1) model a
$$
\begin{align}
    y_{1,t} &= c_1 + \phi _{11,1} y_{1,t-1} + \phi _{12,1} y_{2,t-1} + \varepsilon_{1,t} \\
    y_{2,t} &= c_2 + \phi _{21,1} y_{1,t-1} + \phi _{22,1} y_{2,t-1} + \varepsilon_{2,t},
\end{align}
$$ {#eq-var}
where
    $\varepsilon_{1,t}$ and $\varepsilon_{2,t}$
    are white noise processes
    that may be contemporaneously correlated. </br>
The coefficient $\phi _{ii,\ell}$
    captures the influence of the $\ell$ th lag of variable $y_i$ on itself,
    while the coefficient $\phi _{ij,\ell}$
        captures the influence of the $\ell$ th
        lag of variable $y_j$ on $y_i$.

If the series are stationary,
    we forecast them by fitting a VAR to the data directly
    (known as a **VAR in levels**). </br>
If the series are non-stationary,
    we take differences of the data in order to make them stationary,
    then fit a VAR model (known as a **VAR in differences**). </br>
In both cases, the models are estimated equation by equation
    using the principle of least squares. </br>
For each equation, the parameters are estimated
    by minimising the sum of squared $\varepsilon_{i,t}$ values.

The other possibility,
    which is beyond the scope of this book and therefore we do not explore here,
    is that the series may be non-stationary but **cointegrated**,
        which means that there exists
        a linear combination of them that is stationary. </br>
In this case, a VAR specification that includes an error correction mechanism
    (usually referred to as a **vector error correction model**)
    should be included,
    and alternative estimation methods to least squares estimation should be used.

Forecasts are generated from a VAR in a recursive manner. </br>
The VAR generates forecasts for each variable included in the system. </br>
To illustrate the process,
    assume that we have fitted the 2-dimensional VAR(1) model described in @eq-var,
    for all observations up to time $T$. </br>
Then the one-step-ahead forecasts are generated by
$$
\begin{align*}
    \hat y_{1,T + 1|T} &= \hat{c}_1 + \hat \phi_{11,1} y_{1,T} + \hat \phi_{12,1} y_{2,T} \\
    \hat y_{2,T + 1|T} &= \hat{c}_2 + \hat \phi _{21,1} y_{1,T} + \hat \phi_{22,1} y_{2,T}.
\end{align*}
$$
This is the same form as @eq-var
    except that the errors have been set to zero
    and parameters have been replaced with their estimates. </br>
For $h=2$, the forecasts are given by
$$
\begin{align*}
     \hat y_{1,T + 2|T} &= \hat{c}_1 + \hat \phi_{11,1} \hat y_{1,T + 1|T} + \hat \phi_{12,1} \hat y_{2,T + 1|T}\\
     \hat y_{2,T + 2|T} &= \hat{c}_2 + \hat \phi_{21,1} \hat y_{1,T + 1|T} + \hat \phi_{22,1} \hat y_{2,T + 1|T}. 
\end{align*}
$$
Again, this is the same form as @eq-var,
    except that the errors have been set to zero,
    the parameters have been replaced with their estimates,
    and the unknown values of $y_1$ and $y_2$
        have been replaced with their forecasts. </br>
The process can be iterated in this manner for all future time periods.

There are two decisions one has to make when using a VAR to forecast, namely
    how many variables (denoted by $K$) and
    how many lags (denoted by $p$) should be included in the system. </br>
The number of coefficients to be estimated in a VAR
    is equal to $K + pK^2$ (or $1 + pK$ per equation). </br>
**example**,
    for a VAR with $K=5$ variables and $p=3$ lags,
    there are 16 coefficients per equation,
    giving a total of 80 coefficients to be estimated. </br>
The more coefficients that need to be estimated,
    the larger the estimation error entering the forecast.

In practice, it is usual to keep $K$ small
    and include only variables that are correlated with each other,
    and therefore useful in forecasting each other. </br>
Information criteria are commonly used
    to select the number of lags to be included. </br>
Care should be taken when using the AICc
    as it tends to choose large numbers of lags;
    instead, for VAR models, we often use the BIC instead. </br>
A more sophisticated version of the model is a **sparse VAR**
    (where many coefficients are set to zero);
another approach is to use **shrinkage estimation**
    (where coefficients are smaller).

A criticism that VARs face is that they are atheoretical;
    that is, they are not built on some economic theory
    that imposes a theoretical structure on the equations. </br>
Every variable is assumed to influence every other variable in the system,
    which makes a direct interpretation of the estimated coefficients difficult. </br>
Despite this, VARs are useful in several contexts:
1. forecasting a collection of related variables
    where no explicit interpretation is required;
2. testing whether one variable is useful in forecasting another
    (the basis of **Granger causality tests**);
3. impulse response analysis,
    where the response of one variable
    to a sudden but temporary change in another variable is analysed;
4. forecast error variance decomposition,
    where the proportion of the forecast variance of each variable
    is attributed to the effects of the other variables.


### Example: A VAR model for forecasting US consumption {#sec-eg-us-consumption-var}

```{r}
fit <- us_change %>%
  model(
    aicc = VAR(vars(Consumption, Income)),
    bic = VAR(vars(Consumption, Income), ic = "bic")
  )

glance(fit)
```

A VAR(5) model is selected using the AICc (the default),
    while a VAR(1) model is selected using the BIC. </br>
This is not unusual —
    the BIC will always select a model that has fewer parameters than the AICc model
    as it imposes a stronger penalty for the number of parameters.
```{r}
#| label: fig-us-consumption-var-acf
#| fig-cap: "ACF of the residuals from the two VAR models. A VAR(5) model is selected by the AICc, while a VAR(1) model is selected using the BIC."
fit %>%
  augment() %>%
  ACF(.innov) %>%
  autoplot()
```

We see that the residuals from the VAR(1) model (`bic`)
    have significant autocorrelation for Consumption,
    while the VAR(5) model has effectively captured all the information in the data.

The forecasts generated by the VAR(5) model are plotted in
    @fig-us-consumption-var-forecast.
```{r}
#| label: fig-us-consumption-var-forecast
#| fig-cap: "Forecasts for US consumption and income generated from a VAR(5) model."
fit %>%
  select(aicc) %>%
  forecast() %>%
  autoplot(us_change %>% filter(year(Quarter) > 2010))
```


## Neural network models {#sec-Neural network models}

**Artificial neural networks**
    are forecasting methods that are based on
    simple mathematical models of the brain. </br>
They allow complex nonlinear relationships
    between the response variable and its predictors.


### Neural network architecture {#sec-neural-network-architecture}

A **neural network**
    can be thought of as a network of “neurons”
    which are organised in layers. </br>
The predictors (or inputs) form the bottom layer, and
    the forecasts (or outputs) form the top layer. </br>
There may also be intermediate layers containing “hidden neurons.”

![A simple neural network equivalent to a linear regression.](./neural_net_1.png){#fig-neural-net-1}

The simplest networks contain no hidden layers
    and are equivalent to linear regressions. </br>
@fig-neural-net-1 shows
    the neural network version of a linear regression with four predictors. </br>
The coefficients attached to these predictors
    are called **weights**. </br>
The forecasts are obtained by a linear combination of the inputs. </br>
The weights are selected in the neural network framework
    using a “learning algorithm”
    that minimises a “cost function” such as the MSE. </br>
Of course, in this simple example,
    we can use linear regression
    which is a much more efficient method of training the model.

Once we add an intermediate layer with hidden neurons,
    the neural network becomes non-linear. </br>
A simple example is shown in @fig-neural-net-2.

![A neural network with four inputs and one hidden layer with three hidden neurons.](./neural_net_2.png){#fig-neural-net-2}

This is known as a **multilayer feed-forward network**,
    where each layer of nodes receives inputs from the previous layers. </br>
The outputs of the nodes in one layer
    are inputs to the next layer. </br>
The inputs to each node are combined using a weighted linear combination. </br>
The result is then modified by a nonlinear function before being output. </br>
**example**,
    the inputs into each hidden neuron in @fig-neural-net-2
    are combined linearly to give
    $z_j = b_j + \sum_{i=1}^4 w_{i,j} x_i.$ </br>
In the hidden layer,
    this is then modified using a nonlinear function such as a sigmoid,
    $s(z) = \frac{1}{1 + e^{-z}},$
    to give the input for the next layer. </br>
This tends to reduce the effect of extreme input values,
    thus making the network somewhat robust to outliers.

The parameters
    $b_1, b_2, b_3$ and $w_{1,1}, \dots, w_{4,3}$
    are “learned” (or estimated) from the data. </br>
The values of the weights are often restricted
    to prevent them from becoming too large. </br>
The parameter that restricts the weights is known as the **decay parameter**,
    and is often set to be equal to 0.1.

The weights take random values to begin with,
    and these are then updated using the observed data. </br>
Consequently, there is an element of randomness
    in the predictions produced by a neural network. </br>
Therefore, the network is usually trained several times
    using different random starting points,
    and the results are averaged.

The number of hidden layers,
    and the number of nodes in each hidden layer,
    must be specified in advance. </br>
We will consider how these can be chosen using cross-validation
    later in this chapter.


### Neural network autoregression {#sec-neural-network-autoregression}

With time series data,
    lagged values of the time series can be used as inputs to a neural network,
    just as we used lagged values in a linear autoregression model
    ([Chapter 9](./chapter_9__ARIMA_models.qmd)). </br>
We call this a **neural network autoregression** or **NNAR** model.

In this book, we only consider feed-forward networks with one hidden layer,
    and we use the notation NNAR($p,k$) to indicate 
    there are $p$ lagged inputs
    and $k$ nodes in the hidden layer. </br>
**example**,
    a NNAR(9,5) model is a neural network with the last nine observations
    ($y_{t-1}, y_{t-2}, \dots, y_{t-9}$)
    used as inputs for forecasting the output $y_t$,
    and with five neurons in the hidden layer. </br>
A NNAR($p,0$) model
    is equivalent to an ARIMA($p,0,0$) model,
    but without the restrictions on the parameters to ensure stationarity.

With seasonal data,
    it is useful to also add
    the last observed values from the same season as inputs. </br>
**example**,
    an NNAR(3,1,2)$_{12}$ model has inputs
    $y_{t−1}, y_{t−2}, y_{t−3}$ and $y_{t−12}$,
    and two neurons in the hidden layer. </br>
More generally, an NNAR($p,P,k$)$_m$ model has inputs
    $(y_{t-1}, y_{t-2}, \dots, y_{t-p}, y_{t-m}, y_{t-2m}, \dots, y_{t-Pm})$
    and $k$ neurons in the hidden layer. </br>
A NNAR($p,P,0$)$_m$ model
    is equivalent to an ARIMA($p,0,0$)($P,0,0$)$_m$ model
    but without the restrictions on the parameters that ensure stationarity.

The `NNETAR()` function
    fits an NNAR($p,P,k$)$_m$ model. </br>
If the values of $p$ and $P$ are not specified,
    they are selected automatically. </br>
For non-seasonal time series,
    the default is the optimal number of lags (according to the AIC)
    for a linear AR($p$) model.
For seasonal time series,
    the default values are $P=1$
    and $p$ is chosen from the optimal linear model
        fitted to the seasonally adjusted data. </br>
If $k$ is not specified,
    it is set to $k = (p+P+1) / 2$
    (rounded to the nearest integer).

When it comes to forecasting,
    the network is applied iteratively. </br>
For forecasting one step ahead,
    we simply use the available historical inputs. </br>
For forecasting two steps ahead,
    we use the one-step forecast as an input,
    along with the historical data. </br>
This process proceeds until we have computed all the required forecasts.


### Example: Sunspots {#sec-eg-sunspots}

The surface of the sun contains magnetic regions that appear as dark spots. </br>
These affect the propagation of radio waves,
    and so telecommunication companies like to predict sunspot activity
    in order to plan for any future difficulties. </br>
Sunspots follow a cycle of length between 9 and 14 years. </br>
In @fig-sunspots-forecast-neural-net,
    forecasts from an NNAR(9,5) are shown for the next 30 years. </br>
We have used a square root transformation to ensure the forecasts stay positive.
```{r}
#| label: fig-sunspots-forecast-neural-net
#| fig-cap: "Forecasts from a neural network with nine lagged inputs and one hidden layer containing five neurons."
sunspots <- sunspot.year %>% as_tsibble()

fit <- sunspots %>%
  model(NNETAR(sqrt(value)))

fc <- fit %>% forecast(h = 30)

fc %>%
  autoplot(sunspots) +
  labs(x = "Year", y = "Counts", title = "Yearly sunspots")
```

Here, the last 9 observations are used as predictors,
    and there are 5 neurons in the hidden layer. </br>
The cyclicity in the data has been modelled well. </br>
We can also see the asymmetry of the cycles has been captured by the model,
    where the increasing part of the cycle is steeper than
    the decreasing part of the cycle. </br>
This is one difference between a NNAR model and a linear AR model —
    while linear AR models can model cyclicity,
    the modelled cycles are always symmetric.


### Prediction intervals {#sec-prediction-intervals}

Unlike most of the methods considered in this book,
    neural networks are not based on a well-defined stochastic model,
    and so it is not straightforward to derive prediction intervals
    for the resultant forecasts. </br>
However, we can still compute prediction intervals using simulation
    where future sample paths are generated using bootstrapped residuals
    (as described in
    @sec-distributional-forecasts-prediction-intervals (Chapter 5)).

The neural network fitted to the sunspot data can be written as
    $y_t = f(\bm{y}_{t-1}) + \varepsilon_t$
    where
        $\bm{y}_{t-1} = (y_{t-1}, y_{t-2}, \dots, y_{t-9})'$
            is a vector containing lagged values of the series, and
        $f$ is a neural network with 5 hidden nodes in a single layer. </br>
The error series $\varepsilon_t$
    is assumed to be homoscedastic
    (and possibly also normally distributed).

We can simulate future sample paths of this model iteratively,
    by randomly generating a value for $\varepsilon_t$,
    either from a normal distribution,
    or by resampling from the historical values. </br>
So if $(\varepsilon^*_{T+1}$
    is a random draw from the distribution of errors at time $T+1$,
    then
        $y^*_{T+1} = f(\bm{y}_{T}) + \varepsilon^*_{T+1}$
        is one possible draw from the forecast distribution for $y_{T+1}$. </br>
Setting
    $\bm{y}_{T+1}^* = (y^*_{T+1}, y_{T}, \dots, y_{T-7})'$
    we can then repeat the process to get
    $y^*_{T+2} = f(\bm{y}^*_{T+1}) + \varepsilon^*_{T+2}.$

In this way, we can iteratively simulate a future sample path. </br>
By repeatedly simulating sample paths,
    we build up knowledge of the distribution for all future values
    based on the fitted neural network.

Here is a simulation of 9 possible future sample paths for the sunspot data. </br>
Each sample path covers the next 30 years after the observed data.
```{r}
#| label: fig-sunspot-future-sample-paths
#| fig-cap: "Future sample paths for the annual sunspot data."
fit %>%
  generate(times = 9, h = 30) %>%
  autoplot(.sim) +
  autolayer(sunspots, value) +
  theme(legend.position = "none")
```

If we do this many times,
    we can get a good picture of the forecast distributions. </br>
This is how the `forecast()` function
    produces prediction intervals for NNAR models. </br>
The `times` argument in `forecast()`
    controls how many simulations are done
    (default 1000). </br>
By default, the errors are drawn from a normal distribution. </br>
The `bootstrap` argument
    allows the errors to be “bootstrapped”
    (i.e., randomly drawn from the historical errors).


## Bootstrapping and bagging {#sec-bootstrapping-bagging}


### Bootstrapping time series {#sec-bootstrapping-time-series}

In the preceding section,
    and in @sec-distributional-forecasts-prediction-intervals (Chapter 5),
    we bootstrap the residuals of a time series
    in order to simulate future values of a series using a model.

More generally, we can generate new time series
    that are similar to our observed series,
    using another type of bootstrap.

First, the time series is transformed if necessary,
    and then decomposed into
    trend, seasonal and remainder components using STL. </br>
Then we obtain shuffled versions of the remainder component
    to get bootstrapped remainder series. </br>
Because there may be autocorrelation present in an STL remainder series,
    we cannot simply use the re-draw procedure that was described in
    @sec-distributional-forecasts-prediction-intervals (Chapter 5). </br>
Instead, we use a **blocked bootstrap**,
    where contiguous sections of the time series
    are selected at random and joined together. </br>
These bootstrapped remainder series are added to the trend and seasonal components,
    and the transformation is reversed
    to give variations on the original time series.

Consider the quarterly cement production in Australia
    from 1988 Q1 to 2010 Q2. </br>
First we check, see @fig-cement-production-stl
    that the decomposition has adequately captured the trend and seasonality,
    and that there is no obvious remaining signal in the remainder series.
```{r}
#| label: fig-cement-production-stl
#| fig-cap: "STL decomposition of quarterly Australian cement production."
cement <- aus_production %>%
  filter(year(Quarter) >= 1988) %>%
  select(Quarter, Cement)

cement_stl <- cement %>%
  model(stl = STL(Cement))

cement_stl %>%
  components() %>%
  autoplot()
```

Now we can generate several bootstrapped versions of the data. </br>
Usually, `generate()` produces simulations of the future from a model. </br>
But here we want simulations for the period of the historical data. </br>
So we use the `new_data` argument to pass in the original data
    so that the same time periods are used for the simulated data. </br>
We will use a block size of 8 to cover two years of data.
```{r}
#| label: fig-cement-production-bootstrapped
#| fig-cap: "Ten bootstrapped versions of quarterly Australian cement production (coloured), along with the original data (black)."
cement_stl %>%
  generate(new_data = cement, times = 10, bootstrap_block_size = 8) %>%
  autoplot(.sim) +
  autolayer(cement, Cement) +
  guides(colour = "none") +
  labs(title = "Cement production: Bootstrapped series", y="Tonnes ('000)")
```


### Bagged forecasts {#sec-bagged-forecasts}

One use for these bootstrapped time series is to improve forecast accuracy. </br>
If we produce forecasts from each of the additional time series,
    and average the resulting forecasts,
    we get better forecasts
    than if we simply forecast the original time series directly. </br>
This is called **bagging**
    which stands for **bootstrap aggregating**.

We demonstrate the idea using the cement data. </br>
First, we simulate many time series that are similar to the original data,
    using the block-bootstrap described above.
```{r}
sim <- cement_stl %>%
  generate(new_data = cement, times = 100, bootstrap_block_size = 8) %>%
  select(-.model, -Cement)
```

For each of these series, we fit an ETS model. </br>
A different ETS model may be selected in each case,
    although it will most likely select the same model
    because the series are similar. </br>
However, the estimated parameters will be different,
    so the forecasts will be different even if the selected model is the same. </br>
This is a time-consuming process as there are a large number of series.
```{r}
#| label: fig-cement-production-bootstrapped-forecasts
#| fig-cap: "Forecasts of 100 bootstrapped series obtained using ETS models."
ets_forecasts <- sim %>%
  model(ets = ETS(.sim)) %>%
  forecast(h = 12)

ets_forecasts %>%
  update_tsibble(key = .rep) %>%
  autoplot(.mean) +
  autolayer(cement, Cement) +
  guides(colour = "none") +
  labs(title = "Cement production: bootstrapped forecasts", y = "Tonnes ('000)")
```

Finally, we average these forecasts for each time period
    to obtained the “bagged forecasts” for the original data.
```{r}
#| label: fig-cement-production-ets-forecast
#| fig-cap: "Comparing bagged ETS forecasts (the average of 100 bootstrapped forecasts in orange) and ETS applied directly to the data (in blue)."
bagged <- ets_forecasts %>%
  summarise(bagged_mean = mean(.mean))

cement %>%
  model(ets = ETS(Cement)) %>%
  forecast(h = 12) %>%
  autoplot(cement) +
  autolayer(bagged, bagged_mean, col = "#D55E00") +
  labs(title = "Cement production in Australia", y = "Tonnes ('000)")
```

On average, bagging gives better forecasts
    than just applying `ETS()` directly. </br>
Of course, it is slower
    because a lot more computation is required.


## Exercises


## Future Reading
